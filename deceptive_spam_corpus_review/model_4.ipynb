{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### this model is the model_2 in our report executed on deceptive corpus\"###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of truthfuls reviews  800\n",
      "Number of deceptives reviews  800\n"
     ]
    }
   ],
   "source": [
    "truthful_pos = 'op_spam_v1.4/positive_polarity/truthful_from_TripAdvisor/'\n",
    "truthful_neg = 'op_spam_v1.4/negative_polarity/truthful_from_Web/'\n",
    "\n",
    "deceptive_pos = 'op_spam_v1.4/positive_polarity/deceptive_from_MTurk/'\n",
    "deceptive_neg = 'op_spam_v1.4/negative_polarity/deceptive_from_MTurk/'\n",
    "\n",
    "truthful_reviews_link = []\n",
    "for fold in os.listdir(truthful_pos):\n",
    "    foldLink = os.path.join(truthful_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(truthful_neg):\n",
    "    foldLink = os.path.join(truthful_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "deceptive_reviews_link = []\n",
    "\n",
    "for fold in os.listdir(deceptive_pos):\n",
    "    foldLink = os.path.join(deceptive_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(deceptive_neg):\n",
    "    foldLink = os.path.join(deceptive_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "        \n",
    "print('Number of truthfuls reviews ', len(truthful_reviews_link))\n",
    "print('Number of deceptives reviews ', len(deceptive_reviews_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is  1600\n",
      "The total number of words in the files is  253157\n",
      "Vocabulary size is  9687\n",
      "The average number of words in the files is 158.223125\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def handleFile(filePath):\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        file_voc = []\n",
    "        file_numWords = 0\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            words = cleanedLine.split(' ')\n",
    "            file_numWords = file_numWords + len(words)\n",
    "            file_voc.extend(words)\n",
    "    return file_voc, file_numWords\n",
    "\n",
    "\n",
    "allFilesLinks = truthful_reviews_link + deceptive_reviews_link\n",
    "vocabulary = []\n",
    "numWords = []\n",
    "for fileLink in allFilesLinks:\n",
    "    file_voc, file_numWords = handleFile(fileLink)\n",
    "    vocabulary.extend(file_voc)\n",
    "    numWords.append(file_numWords)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print('The total number of files is ', len(numWords))\n",
    "print('The total number of words in the files is ', sum(numWords))\n",
    "print('Vocabulary size is ', len(vocabulary))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "def convertFileToArray(filePath):\n",
    "    s = \"\"\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            s += cleanedLine\n",
    "    return s\n",
    "\n",
    "totalFiles = len(truthful_reviews_link) + len(deceptive_reviews_link)\n",
    "idsMatrix = np.ndarray(shape=(totalFiles, MAX_SEQ_LENGTH), dtype='int32')\n",
    "#dataMatrix = np.ndarray(shape=(totalFiles,1),dtype='object')\n",
    "dataMatrix = []\n",
    "#labels = np.ndarray(shape=(totalFiles, 2), dtype='int32')\n",
    "labelsMatrix = []\n",
    "counter = 0\n",
    "\n",
    "for filePath in truthful_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(1)\n",
    "  \n",
    "\n",
    "for filePath in deceptive_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(0)\n",
    "\n",
    "dict_reviewLabels = {'review': dataMatrix,'labels': labelsMatrix}\n",
    "df_reviewLabels = pd.DataFrame(dict_reviewLabels)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "macronum=sorted(set(df_reviewLabels['labels']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "df_reviewLabels.iloc[:,0]=df_reviewLabels.iloc[:,0].apply(fun)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "print(df_reviewLabels.shape)\n",
    "# a list contains each review as a list \n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "\n",
    "for i in range(len(df_reviewLabels)):\n",
    "    balanced_texts.append(df_reviewLabels.iloc[i,1])\n",
    "    balanced_labels.append(df_reviewLabels.iloc[i,0])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nhumair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "import re\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#parameters for hierarchical attention network\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\"\"\"\n",
    "train_han = train.dropna()\n",
    "train_han =train_han.reset_index(drop=True)\n",
    "print('Shape of dataset ',train_han.shape)\n",
    "print(train_han.columns)\n",
    "print('No. of unique classes',len(set(train_han['label'])))\n",
    "\n",
    "\n",
    "macronum=sorted(set(train_han['label']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "train_han.iloc[:,0]=train_han.iloc[:,0].apply(fun)\n",
    "# a list contains each review as a list \n",
    "#balanced_texts = []\n",
    "#balanced_labels = []\n",
    "\n",
    "#for i in range(len(train)):\n",
    "   # balanced_texts.append(balanced_train.iloc[i,1])\n",
    "   # balanced_labels.append(balanced_train.iloc[i,0])\n",
    "\"\"\"\n",
    "texts = []\n",
    "labels = []\n",
    "reviews = []\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "for i in range(df_reviewLabels.review.shape[0]):\n",
    "    #text = BeautifulSoup(df.message[i])\n",
    "    text=clean_str(df_reviewLabels.iloc[i,1].lower())\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts),MAX_SENTS, MAX_SENT_LENGTH), dtype= 'int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                        data[i,j,k] = tokenizer.word_index[word]\n",
    "                        k = k+1\n",
    "                except KeyError:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of 9597 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('No of %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1600, 15, 100)\n",
      "Shape of data tensor: (1600, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "labels = []\n",
    "for i in df_reviewLabels['labels']:\n",
    "    labels.append(i)\n",
    "labels_n = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of data tensor:', labels_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels_n = labels_n[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels_n[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels_n[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "cur_dir = os.getcwd()\n",
    "f = open(os.path.join(cur_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "#prepare embedding matrix\n",
    "EMBEDDING_DIM = 100\n",
    "#num_words = min(20000, len(word_index)) + 1\n",
    "#embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "'''\n",
    "for word, i in word_index.items():\n",
    "    if i > 20000:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "'''\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 200)           1120600   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 1,361,802\n",
      "Trainable params: 402,002\n",
      "Non-trainable params: 959,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Bidirectional, TimeDistributed, Flatten, Dense, LSTM\n",
    "from keras.models import Model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype = 'int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "review_input = Input(shape = (MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "preds = Dense(len(macronum),activation='softmax')(l_lstm_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0776 - acc: 0.9742 - val_loss: 1.0284 - val_acc: 0.7719\n",
      "Epoch 2/45\n",
      "1280/1280 [==============================] - 56s 43ms/step - loss: 0.0579 - acc: 0.9805 - val_loss: 1.1791 - val_acc: 0.7094\n",
      "Epoch 3/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0662 - acc: 0.9742 - val_loss: 1.0319 - val_acc: 0.7844\n",
      "Epoch 4/45\n",
      "1280/1280 [==============================] - 56s 43ms/step - loss: 0.0582 - acc: 0.9805 - val_loss: 1.0618 - val_acc: 0.7656\n",
      "Epoch 5/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0490 - acc: 0.9836 - val_loss: 1.0986 - val_acc: 0.7781\n",
      "Epoch 6/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0542 - acc: 0.9828 - val_loss: 0.9701 - val_acc: 0.7500\n",
      "Epoch 7/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0403 - acc: 0.9852 - val_loss: 1.0120 - val_acc: 0.8031\n",
      "Epoch 8/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0431 - acc: 0.9844 - val_loss: 1.3741 - val_acc: 0.7937\n",
      "Epoch 9/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0429 - acc: 0.9844 - val_loss: 1.2354 - val_acc: 0.7687\n",
      "Epoch 10/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0202 - acc: 0.9906 - val_loss: 1.1782 - val_acc: 0.7750\n",
      "Epoch 11/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0359 - acc: 0.9922 - val_loss: 1.2425 - val_acc: 0.7812\n",
      "Epoch 12/45\n",
      "1280/1280 [==============================] - 56s 43ms/step - loss: 0.0339 - acc: 0.9867 - val_loss: 1.1911 - val_acc: 0.7531\n",
      "Epoch 13/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0186 - acc: 0.9945 - val_loss: 1.5896 - val_acc: 0.7625\n",
      "Epoch 14/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0371 - acc: 0.9906 - val_loss: 1.5386 - val_acc: 0.7469\n",
      "Epoch 15/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0224 - acc: 0.9953 - val_loss: 1.7912 - val_acc: 0.7500\n",
      "Epoch 16/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0286 - acc: 0.9922 - val_loss: 1.5706 - val_acc: 0.7906\n",
      "Epoch 17/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0230 - acc: 0.9953 - val_loss: 1.8024 - val_acc: 0.7656\n",
      "Epoch 18/45\n",
      "1280/1280 [==============================] - 56s 43ms/step - loss: 0.0225 - acc: 0.9937 - val_loss: 1.6279 - val_acc: 0.7750\n",
      "Epoch 19/45\n",
      "1280/1280 [==============================] - 56s 44ms/step - loss: 0.0223 - acc: 0.9914 - val_loss: 1.6140 - val_acc: 0.7750\n",
      "Epoch 20/45\n",
      "1180/1280 [==========================>...] - ETA: 4s - loss: 0.0236 - acc: 0.9949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-01a39bb5cb6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train, validation_data = (x_val, y_val),epochs=45,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9906\n",
      "Testing Accuracy:  0.7469\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "python_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of truthfuls reviews  800\n",
      "Number of deceptives reviews  800\n"
     ]
    }
   ],
   "source": [
    "truthful_pos = 'op_spam_v1.4/positive_polarity/truthful_from_TripAdvisor/'\n",
    "truthful_neg = 'op_spam_v1.4/negative_polarity/truthful_from_Web/'\n",
    "\n",
    "deceptive_pos = 'op_spam_v1.4/positive_polarity/deceptive_from_MTurk/'\n",
    "deceptive_neg = 'op_spam_v1.4/negative_polarity/deceptive_from_MTurk/'\n",
    "\n",
    "truthful_reviews_link = []\n",
    "for fold in os.listdir(truthful_pos):\n",
    "    foldLink = os.path.join(truthful_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(truthful_neg):\n",
    "    foldLink = os.path.join(truthful_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "deceptive_reviews_link = []\n",
    "\n",
    "for fold in os.listdir(deceptive_pos):\n",
    "    foldLink = os.path.join(deceptive_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(deceptive_neg):\n",
    "    foldLink = os.path.join(deceptive_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "        \n",
    "print('Number of truthfuls reviews ', len(truthful_reviews_link))\n",
    "print('Number of deceptives reviews ', len(deceptive_reviews_link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is  1600\n",
      "The total number of words in the files is  253157\n",
      "Vocabulary size is  9687\n",
      "The average number of words in the files is 158.223125\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def handleFile(filePath):\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        file_voc = []\n",
    "        file_numWords = 0\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            words = cleanedLine.split(' ')\n",
    "            file_numWords = file_numWords + len(words)\n",
    "            file_voc.extend(words)\n",
    "    return file_voc, file_numWords\n",
    "\n",
    "\n",
    "allFilesLinks = truthful_reviews_link + deceptive_reviews_link\n",
    "vocabulary = []\n",
    "numWords = []\n",
    "for fileLink in allFilesLinks:\n",
    "    file_voc, file_numWords = handleFile(fileLink)\n",
    "    vocabulary.extend(file_voc)\n",
    "    numWords.append(file_numWords)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print('The total number of files is ', len(numWords))\n",
    "print('The total number of words in the files is ', sum(numWords))\n",
    "print('Vocabulary size is ', len(vocabulary))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "def convertFileToArray(filePath):\n",
    "    s = \"\"\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            s += cleanedLine\n",
    "    return s\n",
    "\n",
    "totalFiles = len(truthful_reviews_link) + len(deceptive_reviews_link)\n",
    "idsMatrix = np.ndarray(shape=(totalFiles, MAX_SEQ_LENGTH), dtype='int32')\n",
    "#dataMatrix = np.ndarray(shape=(totalFiles,1),dtype='object')\n",
    "dataMatrix = []\n",
    "#labels = np.ndarray(shape=(totalFiles, 2), dtype='int32')\n",
    "labelsMatrix = []\n",
    "counter = 0\n",
    "\n",
    "for filePath in truthful_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(1)\n",
    "  \n",
    "\n",
    "for filePath in deceptive_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(0)\n",
    "\n",
    "dict_reviewLabels = {'review': dataMatrix,'labels': labelsMatrix}\n",
    "df_reviewLabels = pd.DataFrame(dict_reviewLabels)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "macronum=sorted(set(df_reviewLabels['labels']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "df_reviewLabels.iloc[:,0]=df_reviewLabels.iloc[:,0].apply(fun)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "print(df_reviewLabels.shape)\n",
    "# a list contains each review as a list \n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "\n",
    "for i in range(len(df_reviewLabels)):\n",
    "    balanced_texts.append(df_reviewLabels.iloc[i,1])\n",
    "    balanced_labels.append(df_reviewLabels.iloc[i,0])\n",
    " \n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \")#20000\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "x = pad_sequences(sequences, maxlen=200)#300\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(balanced_labels))\n",
    "#y = df_reviewLabels['labels'].values\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, GlobalMaxPooling1D, Conv1D, Dropout, MaxPooling1D, Dense, Embedding, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "# Build embedding layers with weights initialized from each model\n",
    "googlenews_w2v_size = 300\n",
    "googlenews_w2v_matrix = np.zeros((len(word_index) + 1, googlenews_w2v_size))\n",
    "for word,i in word_index.items():\n",
    "    try:\n",
    "        if word in w2v_model.vocab:\n",
    "            googlenews_w2v_matrix[i] = w2v_model[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "googlenews_w2v_emb = Embedding(len(word_index)+1,\n",
    "\n",
    "                            googlenews_w2v_size,\n",
    "\n",
    "                            weights=[googlenews_w2v_matrix],\n",
    "\n",
    "                            input_length=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * x.shape[0])\n",
    "\n",
    "x_train = x[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = x[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20)                19260     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,924,202\n",
      "Trainable params: 2,924,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6961 - acc: 0.4935 - val_loss: 0.6877 - val_acc: 0.5547\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6851 - acc: 0.5846 - val_loss: 0.6833 - val_acc: 0.5645\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6740 - acc: 0.6497 - val_loss: 0.6790 - val_acc: 0.5937\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6624 - acc: 0.6706 - val_loss: 0.6746 - val_acc: 0.6152\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6453 - acc: 0.7096 - val_loss: 0.6696 - val_acc: 0.6035\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6354 - acc: 0.7227 - val_loss: 0.6646 - val_acc: 0.6074\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6207 - acc: 0.7383 - val_loss: 0.6595 - val_acc: 0.6074\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6002 - acc: 0.7435 - val_loss: 0.6540 - val_acc: 0.6113\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5791 - acc: 0.7565 - val_loss: 0.6481 - val_acc: 0.6133\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5588 - acc: 0.7656 - val_loss: 0.6418 - val_acc: 0.6191\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5291 - acc: 0.7773 - val_loss: 0.6353 - val_acc: 0.6309\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5011 - acc: 0.7995 - val_loss: 0.6280 - val_acc: 0.6367\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.4695 - acc: 0.8216 - val_loss: 0.6210 - val_acc: 0.6523\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.4259 - acc: 0.8607 - val_loss: 0.6098 - val_acc: 0.6621\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.3858 - acc: 0.8555 - val_loss: 0.6057 - val_acc: 0.6699\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.3409 - acc: 0.8945 - val_loss: 0.6026 - val_acc: 0.6777\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.2928 - acc: 0.9115 - val_loss: 0.6093 - val_acc: 0.6953\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.2456 - acc: 0.9310 - val_loss: 0.6281 - val_acc: 0.7090\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.2088 - acc: 0.9336 - val_loss: 0.6632 - val_acc: 0.7090\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1765 - acc: 0.9531 - val_loss: 0.6848 - val_acc: 0.7129\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1508 - acc: 0.9648 - val_loss: 0.6771 - val_acc: 0.7168\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1258 - acc: 0.9701 - val_loss: 0.7001 - val_acc: 0.7148\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1118 - acc: 0.9766 - val_loss: 0.6969 - val_acc: 0.7285\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0956 - acc: 0.9831 - val_loss: 0.7223 - val_acc: 0.7246\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0936 - acc: 0.9831 - val_loss: 0.7440 - val_acc: 0.7246\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0860 - acc: 0.9831 - val_loss: 0.7475 - val_acc: 0.7324\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0807 - acc: 0.9857 - val_loss: 0.7762 - val_acc: 0.7246\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0767 - acc: 0.9870 - val_loss: 0.7764 - val_acc: 0.7227\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0689 - acc: 0.9870 - val_loss: 0.7877 - val_acc: 0.7266\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0671 - acc: 0.9870 - val_loss: 0.8188 - val_acc: 0.7109\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0628 - acc: 0.9896 - val_loss: 0.8261 - val_acc: 0.7168\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0542 - acc: 0.9909 - val_loss: 0.8480 - val_acc: 0.7168\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0525 - acc: 0.9896 - val_loss: 1.0060 - val_acc: 0.6836\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0678 - acc: 0.9870 - val_loss: 0.8174 - val_acc: 0.7051\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0637 - acc: 0.9844 - val_loss: 0.8296 - val_acc: 0.7109\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0620 - acc: 0.9883 - val_loss: 0.9298 - val_acc: 0.7168\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0435 - acc: 0.9935 - val_loss: 0.8651 - val_acc: 0.7168\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0422 - acc: 0.9935 - val_loss: 0.8872 - val_acc: 0.7187\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0356 - acc: 0.9935 - val_loss: 0.9431 - val_acc: 0.7168\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0354 - acc: 0.9948 - val_loss: 0.9630 - val_acc: 0.7148\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0291 - acc: 0.9961 - val_loss: 0.9149 - val_acc: 0.7168\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0297 - acc: 0.9961 - val_loss: 0.9226 - val_acc: 0.7070\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0258 - acc: 0.9961 - val_loss: 0.9438 - val_acc: 0.7070\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0252 - acc: 0.9961 - val_loss: 0.9671 - val_acc: 0.7070\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0301 - acc: 0.9948 - val_loss: 0.9899 - val_acc: 0.7109\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "#GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(GRU(units=20,activation='tanh',recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,batch_size=200,epochs=45,validation_split=0.4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8820\n",
      "Testing Accuracy:  0.7031\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 82        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,956,342\n",
      "Trainable params: 2,956,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 5s 6ms/step - loss: 0.6888 - acc: 0.5859 - val_loss: 0.6903 - val_acc: 0.5352\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6803 - acc: 0.6536 - val_loss: 0.6871 - val_acc: 0.5723\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6718 - acc: 0.7266 - val_loss: 0.6836 - val_acc: 0.5957\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6606 - acc: 0.7565 - val_loss: 0.6793 - val_acc: 0.6074\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6478 - acc: 0.7839 - val_loss: 0.6729 - val_acc: 0.6250\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6250 - acc: 0.8333 - val_loss: 0.6636 - val_acc: 0.6426\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.5981 - acc: 0.8581 - val_loss: 0.6499 - val_acc: 0.6660\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.5544 - acc: 0.8737 - val_loss: 0.6295 - val_acc: 0.6953\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.4939 - acc: 0.9167 - val_loss: 0.5891 - val_acc: 0.7344\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.4057 - acc: 0.9544 - val_loss: 0.5486 - val_acc: 0.7500\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.3538 - acc: 0.9323 - val_loss: 0.5425 - val_acc: 0.7422\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.3187 - acc: 0.9427 - val_loss: 0.5280 - val_acc: 0.7598\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.2650 - acc: 0.9792 - val_loss: 0.5320 - val_acc: 0.7480\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.2284 - acc: 0.9805 - val_loss: 0.5183 - val_acc: 0.7520\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1989 - acc: 0.9844 - val_loss: 0.5189 - val_acc: 0.7559\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1717 - acc: 0.9844 - val_loss: 0.5277 - val_acc: 0.7559\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1434 - acc: 0.9844 - val_loss: 0.5366 - val_acc: 0.7539\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1274 - acc: 0.9883 - val_loss: 0.5490 - val_acc: 0.7617\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1129 - acc: 0.9896 - val_loss: 0.5615 - val_acc: 0.7559\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0922 - acc: 0.9935 - val_loss: 0.5955 - val_acc: 0.7422\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0810 - acc: 0.9961 - val_loss: 0.5984 - val_acc: 0.7520\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0676 - acc: 0.9974 - val_loss: 0.6129 - val_acc: 0.7598\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0610 - acc: 0.9961 - val_loss: 0.6348 - val_acc: 0.7539\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0550 - acc: 0.9974 - val_loss: 0.6505 - val_acc: 0.7520\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0462 - acc: 0.9974 - val_loss: 0.6710 - val_acc: 0.7559\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0430 - acc: 0.9974 - val_loss: 0.6899 - val_acc: 0.7539\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0416 - acc: 0.9974 - val_loss: 0.7136 - val_acc: 0.7539\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0410 - acc: 0.9974 - val_loss: 0.7292 - val_acc: 0.7539\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0346 - acc: 0.9974 - val_loss: 0.7480 - val_acc: 0.7520\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0316 - acc: 0.9974 - val_loss: 0.7773 - val_acc: 0.7402\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0299 - acc: 0.9974 - val_loss: 0.7854 - val_acc: 0.7402\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0293 - acc: 0.9974 - val_loss: 0.8039 - val_acc: 0.7305\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0276 - acc: 0.9974 - val_loss: 0.8126 - val_acc: 0.7305\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0253 - acc: 0.9974 - val_loss: 0.8181 - val_acc: 0.7461\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0251 - acc: 0.9987 - val_loss: 0.8283 - val_acc: 0.7441\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0224 - acc: 0.9987 - val_loss: 0.8692 - val_acc: 0.7266\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0257 - acc: 0.9948 - val_loss: 0.8083 - val_acc: 0.7520\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0249 - acc: 0.9974 - val_loss: 0.8830 - val_acc: 0.7305\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.8489 - val_acc: 0.7324\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.8387 - val_acc: 0.7402\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.8672 - val_acc: 0.7441\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.8848 - val_acc: 0.7461\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.8805 - val_acc: 0.7441\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.8809 - val_acc: 0.7422\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.8814 - val_acc: 0.7402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15245ed58128>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM, Bidirectional\n",
    "#Bi directional LSTM\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Bidirectional(LSTM(units=20,activation='tanh',recurrent_activation='hard_sigmoid')))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, nb_epoch=45,validation_split=0.4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8961\n",
      "Testing Accuracy:  0.7156\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20)                25680     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,930,622\n",
      "Trainable params: 2,930,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:15: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 3s 4ms/step - loss: 0.6924 - acc: 0.4987 - val_loss: 0.6855 - val_acc: 0.5781\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6734 - acc: 0.6484 - val_loss: 0.6779 - val_acc: 0.6035\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6502 - acc: 0.7435 - val_loss: 0.6688 - val_acc: 0.6348\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6208 - acc: 0.8164 - val_loss: 0.6568 - val_acc: 0.6426\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.5857 - acc: 0.8451 - val_loss: 0.6401 - val_acc: 0.6641\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.5332 - acc: 0.8841 - val_loss: 0.6174 - val_acc: 0.6797\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.4659 - acc: 0.8919 - val_loss: 0.5854 - val_acc: 0.7070\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.3664 - acc: 0.9440 - val_loss: 0.5460 - val_acc: 0.7383\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.2472 - acc: 0.9805 - val_loss: 0.5359 - val_acc: 0.7480\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1978 - acc: 0.9674 - val_loss: 0.6072 - val_acc: 0.7051\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1797 - acc: 0.9596 - val_loss: 0.5769 - val_acc: 0.7402\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1419 - acc: 0.9805 - val_loss: 0.5528 - val_acc: 0.7578\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1174 - acc: 0.9922 - val_loss: 0.5631 - val_acc: 0.7539\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1168 - acc: 0.9922 - val_loss: 0.5820 - val_acc: 0.7461\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0943 - acc: 0.9948 - val_loss: 0.6271 - val_acc: 0.7422\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0716 - acc: 0.9961 - val_loss: 0.6318 - val_acc: 0.7480\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0613 - acc: 0.9974 - val_loss: 0.6645 - val_acc: 0.7461\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0601 - acc: 0.9935 - val_loss: 0.7149 - val_acc: 0.7402\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0547 - acc: 0.9948 - val_loss: 0.6865 - val_acc: 0.7539\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0501 - acc: 0.9948 - val_loss: 0.7096 - val_acc: 0.7598\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0460 - acc: 0.9974 - val_loss: 0.7381 - val_acc: 0.7422\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0369 - acc: 0.9974 - val_loss: 0.7692 - val_acc: 0.7402\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0355 - acc: 0.9987 - val_loss: 0.7667 - val_acc: 0.7500\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0320 - acc: 0.9987 - val_loss: 0.7542 - val_acc: 0.7539\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.7623 - val_acc: 0.7578\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.8110 - val_acc: 0.7422\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0251 - acc: 0.9974 - val_loss: 0.8489 - val_acc: 0.7363\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.8384 - val_acc: 0.7422\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0270 - acc: 0.9974 - val_loss: 0.8418 - val_acc: 0.7402\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0235 - acc: 0.9974 - val_loss: 0.8167 - val_acc: 0.7520\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.8504 - val_acc: 0.7500\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.8894 - val_acc: 0.7344\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0206 - acc: 0.9987 - val_loss: 0.8911 - val_acc: 0.7441\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.8556 - val_acc: 0.7500\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.8693 - val_acc: 0.7480\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.8877 - val_acc: 0.7441\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.8969 - val_acc: 0.7441\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.9107 - val_acc: 0.7559\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.9545 - val_acc: 0.7480\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.9898 - val_acc: 0.7383\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 1.0109 - val_acc: 0.7344\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 1.0168 - val_acc: 0.7383\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0150 - acc: 0.9987 - val_loss: 1.0305 - val_acc: 0.7305\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0284 - acc: 0.9961 - val_loss: 0.9233 - val_acc: 0.7578\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.9677 - val_acc: 0.7422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15245cdd84e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units=20,activation='tanh',recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, nb_epoch=45,validation_split=0.4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8969\n",
      "Testing Accuracy:  0.7250\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(strides=1, activation=\"relu\", padding=\"same\", kernel_size=3, filters=150)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 150)          135150    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 150)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 20)                13680     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,053,772\n",
      "Trainable params: 3,053,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 3s 4ms/step - loss: 0.6868 - acc: 0.5651 - val_loss: 0.6721 - val_acc: 0.6367\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.6216 - acc: 0.8359 - val_loss: 0.6472 - val_acc: 0.6895\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.5430 - acc: 0.8724 - val_loss: 0.6148 - val_acc: 0.6836\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.4326 - acc: 0.8984 - val_loss: 0.5742 - val_acc: 0.7090\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.2915 - acc: 0.9466 - val_loss: 0.5272 - val_acc: 0.7461\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.1843 - acc: 0.9805 - val_loss: 0.4886 - val_acc: 0.7793\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.1339 - acc: 0.9792 - val_loss: 0.4894 - val_acc: 0.7930\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.1032 - acc: 0.9844 - val_loss: 0.5313 - val_acc: 0.7754\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0787 - acc: 0.9896 - val_loss: 0.5747 - val_acc: 0.7598\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0575 - acc: 0.9948 - val_loss: 0.5966 - val_acc: 0.7578\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0522 - acc: 0.9922 - val_loss: 0.6717 - val_acc: 0.7617\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.6576 - val_acc: 0.7656\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0362 - acc: 0.9987 - val_loss: 0.6671 - val_acc: 0.7676\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 1.0000 - val_loss: 0.6972 - val_acc: 0.7676\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.9987 - val_loss: 0.7326 - val_acc: 0.7734\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.7443 - val_acc: 0.7734\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.7623 - val_acc: 0.7754\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.7770 - val_acc: 0.7656\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.7901 - val_acc: 0.7695\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.8014 - val_acc: 0.7715\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.8214 - val_acc: 0.7734\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0156 - acc: 0.9987 - val_loss: 0.9109 - val_acc: 0.7578\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0145 - acc: 0.9987 - val_loss: 0.8185 - val_acc: 0.7793\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9987 - val_loss: 0.8350 - val_acc: 0.7793\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0157 - acc: 0.9987 - val_loss: 0.8454 - val_acc: 0.7715\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.7671 - val_acc: 0.7422\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 1.0000 - val_loss: 0.8309 - val_acc: 0.7598\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.8209 - val_acc: 0.7559\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.8126 - val_acc: 0.7637\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.8511 - val_acc: 0.7656\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.8738 - val_acc: 0.7695\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.8959 - val_acc: 0.7637\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0160 - acc: 0.9974 - val_loss: 0.8771 - val_acc: 0.7695\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0104 - acc: 0.9987 - val_loss: 0.9319 - val_acc: 0.7637\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0104 - acc: 0.9987 - val_loss: 0.8845 - val_acc: 0.7637\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.8968 - val_acc: 0.7656\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0105 - acc: 0.9987 - val_loss: 0.9378 - val_acc: 0.7695\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.7520\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0143 - acc: 0.9974 - val_loss: 1.0254 - val_acc: 0.7461\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0645 - acc: 0.9805 - val_loss: 1.1359 - val_acc: 0.7305\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0408 - acc: 0.9831 - val_loss: 1.0905 - val_acc: 0.6992\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0601 - acc: 0.9792 - val_loss: 0.9016 - val_acc: 0.7363\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0585 - acc: 0.9753 - val_loss: 0.8279 - val_acc: 0.7539\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0267 - acc: 0.9935 - val_loss: 0.9722 - val_acc: 0.7441\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0147 - acc: 0.9974 - val_loss: 0.8858 - val_acc: 0.7754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15245c0e9390>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN + LSTM\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution1D(nb_filter=150,\n",
    "                        filter_length=3,\n",
    "                        border_mode='same',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(units=20,activation='tanh',recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, epochs=45,validation_split=0.4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9102\n",
      "Testing Accuracy:  0.7625\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, kernel_size=3, filters=100)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, kernel_size=4, filters=100)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, kernel_size=5, filters=100)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 29500)             360300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1888064   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 5,153,394\n",
      "Trainable params: 5,153,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[200,200,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/zeros_2 = Fill[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/Shape_3, training/Adam/gradients/zeros_2/Const)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1106_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training/Adam/gradients/zeros_2', defined at:\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2907, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-b5061e4cd4f1>\", line 48, in <module>\n    model.fit(x_train, y_train, batch_size=200, epochs=45,validation_split=0.4,shuffle=True)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1010, in fit\n    self._make_train_function()\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 509, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 475, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 89, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2757, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 600, in gradients\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1365, in ZerosLikeOutsideLoop\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1510, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1801, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[200,200,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/zeros_2 = Fill[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/Shape_3, training/Adam/gradients/zeros_2/Const)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1106_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[200,200,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/zeros_2 = Fill[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/Shape_3, training/Adam/gradients/zeros_2/Const)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1106_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b5061e4cd4f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[200,200,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/zeros_2 = Fill[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/Shape_3, training/Adam/gradients/zeros_2/Const)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1106_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training/Adam/gradients/zeros_2', defined at:\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2907, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-b5061e4cd4f1>\", line 48, in <module>\n    model.fit(x_train, y_train, batch_size=200, epochs=45,validation_split=0.4,shuffle=True)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1010, in fit\n    self._make_train_function()\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 509, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 475, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 89, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/nhumair/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2757, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 600, in gradients\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1365, in ZerosLikeOutsideLoop\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1510, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1801, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[200,200,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/zeros_2 = Fill[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/Shape_3, training/Adam/gradients/zeros_2/Const)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1106_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# CNN\n",
    "# Based on \"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "# https://github.com/keon/keras-text-classification/blob/master/train.py\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import SGD\n",
    "filter_sizes = (3,4,5)\n",
    "num_filters = 100\n",
    "graph_in = Input(shape=(200, googlenews_w2v_size))\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Convolution1D(nb_filter=100,\n",
    "                         filter_length=fsz,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    pool = MaxPooling1D(pool_length=2)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    convs.append(flatten)\n",
    "\n",
    "if len(filter_sizes) > 1:\n",
    "    out = concatenate(convs)\n",
    "    #out = Merge(mode='concat')(convs)\n",
    "else:\n",
    "    out = convs[0]\n",
    "\n",
    "graph = Model(input=graph_in, output=out)\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.25, input_shape=(200, googlenews_w2v_size)))\n",
    "model.add(graph)\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "opt = SGD(lr=0.01, momentum=0.80, decay=1e-6, nesterov=True)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, epochs=45,validation_split=0.4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9164\n",
      "Testing Accuracy:  0.7812\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### model_1#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 200, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 36, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 3, 128)            82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,277,894\n",
      "Trainable params: 3,277,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.7457 - acc: 0.5104 - val_loss: 0.6932 - val_acc: 0.4941\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.6947 - acc: 0.4948 - val_loss: 0.6941 - val_acc: 0.5059\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.6961 - acc: 0.4922 - val_loss: 0.6915 - val_acc: 0.5039\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.6888 - acc: 0.5482 - val_loss: 0.6920 - val_acc: 0.5059\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.6790 - acc: 0.5807 - val_loss: 0.6694 - val_acc: 0.5254\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.6885 - acc: 0.5352 - val_loss: 0.6564 - val_acc: 0.6660\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.6417 - acc: 0.6576 - val_loss: 0.6488 - val_acc: 0.6484\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.6109 - acc: 0.6771 - val_loss: 0.6274 - val_acc: 0.6348\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.5610 - acc: 0.7344 - val_loss: 0.6295 - val_acc: 0.6953\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.6057 - acc: 0.6901 - val_loss: 0.5890 - val_acc: 0.6699\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.4714 - acc: 0.7891 - val_loss: 0.6202 - val_acc: 0.7051\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.6340 - acc: 0.7018 - val_loss: 0.5441 - val_acc: 0.7070\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4028 - acc: 0.8411 - val_loss: 0.5128 - val_acc: 0.7480\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4269 - acc: 0.8268 - val_loss: 0.7469 - val_acc: 0.5898\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.3895 - acc: 0.8359 - val_loss: 0.4708 - val_acc: 0.7656\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.2868 - acc: 0.8945 - val_loss: 1.3628 - val_acc: 0.5391\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.4857 - acc: 0.8190 - val_loss: 0.4658 - val_acc: 0.7695\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.2085 - acc: 0.9310 - val_loss: 0.5977 - val_acc: 0.7285\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3605 - acc: 0.8281 - val_loss: 0.4671 - val_acc: 0.7812\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.1647 - acc: 0.9492 - val_loss: 0.4839 - val_acc: 0.7852\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4033 - acc: 0.8542 - val_loss: 0.8317 - val_acc: 0.6250\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.2379 - acc: 0.9063 - val_loss: 0.4520 - val_acc: 0.7949\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.1064 - acc: 0.9805 - val_loss: 0.4695 - val_acc: 0.8184\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.0688 - acc: 0.9883 - val_loss: 0.5183 - val_acc: 0.8125\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.0402 - acc: 0.9896 - val_loss: 0.5770 - val_acc: 0.8184\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.0205 - acc: 0.9987 - val_loss: 0.6777 - val_acc: 0.8242\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.0170 - acc: 0.9987 - val_loss: 2.6519 - val_acc: 0.6172\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 0s 429us/step - loss: 1.4535 - acc: 0.7839 - val_loss: 0.4681 - val_acc: 0.8203\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.0401 - acc: 1.0000 - val_loss: 0.4998 - val_acc: 0.8203\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.5385 - val_acc: 0.8203\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.5860 - val_acc: 0.8184\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.6137 - val_acc: 0.8223\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.6847 - val_acc: 0.8223\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.7153 - val_acc: 0.8262\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.7432 - val_acc: 0.8320\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.7794 - val_acc: 0.8301\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7651 - val_acc: 0.8262\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7921 - val_acc: 0.8281\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.8373 - val_acc: 0.8301\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 1.4575 - acc: 0.8646 - val_loss: 0.6456 - val_acc: 0.8223\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.6491 - val_acc: 0.8301\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.0074 - acc: 0.9987 - val_loss: 0.6921 - val_acc: 0.8262\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.7117 - val_acc: 0.8262\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.7498 - val_acc: 0.8242\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7952 - val_acc: 0.8242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14dabbbf5668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Dropout(0.2)(embedded_sequences)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(macronum), activation='sigmoid')(x)\n",
    "#preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution1D(nb_filter=150,\n",
    "                        filter_length=3,\n",
    "                        border_mode='same',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\"\"\"\n",
    "\n",
    "################################\n",
    "from keras.layers import Convolution1D\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.25, input_shape=(200, googlenews_w2v_size)))\n",
    "#model.add(graph)\n",
    "model.add(Convolution1D(activation='relu',padding=\"same\",strides=1,kernel_size=5,filters=128))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Convolution1D(128,5,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Convolution1D(128,5,activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(len(macronum),activation='sigmoid'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "#opt = SGD(lr=0.01, momentum=0.80, decay=1e-6, nesterov=True)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, epochs=45,validation_split=0.4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9297\n",
      "Testing Accuracy:  0.8031\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[1.17081450e-04 9.34783518e-01]\n",
      " [9.99370396e-01 2.24625666e-07]\n",
      " [7.34363019e-01 2.42865877e-03]\n",
      " [3.93578187e-02 2.84656078e-01]\n",
      " [9.85269487e-01 2.59092267e-05]\n",
      " [2.54475530e-02 8.19225669e-01]\n",
      " [2.15085838e-05 9.88842845e-01]\n",
      " [5.90993383e-04 7.40033388e-01]\n",
      " [5.92930675e-01 5.37800370e-03]\n",
      " [9.05126572e-01 4.70391096e-04]\n",
      " [1.83282763e-01 8.99639651e-02]\n",
      " [9.92420256e-01 1.54334703e-05]\n",
      " [8.52819794e-05 9.70091164e-01]\n",
      " [9.97386873e-01 4.47307275e-06]\n",
      " [8.11310530e-01 1.84493512e-02]\n",
      " [4.93406534e-01 9.96441990e-02]\n",
      " [6.26584952e-05 9.79695082e-01]\n",
      " [1.73495820e-04 9.57272232e-01]\n",
      " [9.83135998e-01 1.10953442e-04]\n",
      " [9.17526777e-04 8.50658596e-01]\n",
      " [9.99421954e-01 2.25162069e-07]\n",
      " [9.87893283e-01 1.34061092e-05]\n",
      " [9.95366454e-01 7.40433507e-06]\n",
      " [9.99371946e-01 4.90957817e-08]\n",
      " [7.87345110e-04 8.44405949e-01]\n",
      " [3.27637307e-02 4.82896179e-01]\n",
      " [2.25792686e-03 3.83941323e-01]\n",
      " [1.85996853e-02 7.04358399e-01]\n",
      " [1.27186291e-02 7.24657550e-02]\n",
      " [1.16187662e-01 1.32208439e-02]\n",
      " [4.01478837e-06 9.95612741e-01]\n",
      " [4.70391482e-01 1.40279397e-01]\n",
      " [6.51141090e-05 9.80246902e-01]\n",
      " [1.42305294e-06 9.97003496e-01]\n",
      " [4.63724369e-03 7.25766540e-01]\n",
      " [4.18782525e-04 2.64784008e-01]\n",
      " [8.40244931e-04 4.38232630e-01]\n",
      " [8.92930031e-01 1.71278894e-03]\n",
      " [5.87210894e-01 8.54634866e-02]\n",
      " [9.71412122e-01 7.31246109e-05]\n",
      " [2.48902215e-04 9.53923106e-01]\n",
      " [8.83819163e-01 4.67155885e-04]\n",
      " [4.17810708e-01 1.21200070e-01]\n",
      " [3.93589050e-01 1.36623753e-03]\n",
      " [5.25463372e-04 8.28486800e-01]\n",
      " [3.43823718e-04 8.81177843e-01]\n",
      " [9.91412759e-01 1.50537574e-06]\n",
      " [1.97238639e-01 3.56256753e-01]\n",
      " [5.10722816e-01 1.64141152e-02]\n",
      " [9.95704949e-01 2.51119650e-06]\n",
      " [4.62940152e-05 9.77544904e-01]\n",
      " [1.88965648e-02 5.65260410e-01]\n",
      " [7.04370677e-01 4.45613861e-02]\n",
      " [9.99373853e-01 1.07697275e-07]\n",
      " [2.50231387e-04 9.03638303e-01]\n",
      " [2.85019070e-01 2.69234832e-02]\n",
      " [5.06700063e-03 8.16252708e-01]\n",
      " [2.49378942e-03 8.29786003e-01]\n",
      " [9.54114497e-01 6.42518571e-04]\n",
      " [5.10452807e-01 5.34265302e-03]\n",
      " [1.12912923e-01 1.42679568e-02]\n",
      " [9.98646557e-01 3.39727535e-07]\n",
      " [3.50344228e-04 9.63171005e-01]\n",
      " [9.98304844e-01 7.46349372e-07]\n",
      " [2.53139347e-01 1.56478714e-02]\n",
      " [3.01829964e-01 1.28159765e-02]\n",
      " [2.95663197e-02 2.71347702e-01]\n",
      " [9.96329248e-01 2.93032144e-06]\n",
      " [1.01369196e-05 9.93777514e-01]\n",
      " [5.65519094e-01 7.42108049e-03]\n",
      " [1.15495175e-02 8.70399475e-01]\n",
      " [8.92910242e-01 4.08598396e-04]\n",
      " [1.28306301e-05 9.81358111e-01]\n",
      " [1.55036178e-04 9.33470845e-01]\n",
      " [9.95522380e-01 2.68255235e-06]\n",
      " [7.75369033e-02 2.28077755e-03]\n",
      " [6.36369467e-01 7.12886103e-04]\n",
      " [9.92165208e-01 1.74021152e-05]\n",
      " [9.96650040e-01 3.25225096e-06]\n",
      " [7.15109184e-02 5.95066436e-02]\n",
      " [1.94753651e-02 7.20162749e-01]\n",
      " [4.64099571e-02 1.67521220e-02]\n",
      " [1.46538555e-03 9.40695405e-01]\n",
      " [8.56427960e-06 9.83997226e-01]\n",
      " [1.72238871e-02 1.96399480e-01]\n",
      " [1.18765950e-01 4.95823696e-02]\n",
      " [9.35396194e-01 4.48181876e-04]\n",
      " [5.05790830e-01 1.75584815e-02]\n",
      " [7.83715863e-04 9.48025465e-01]\n",
      " [4.17202798e-04 5.84805131e-01]\n",
      " [6.02900796e-03 4.51707900e-01]\n",
      " [1.14847266e-03 7.15641975e-01]\n",
      " [9.94411528e-01 1.00449697e-05]\n",
      " [4.42418218e-01 1.23881539e-02]\n",
      " [3.08533888e-02 4.42404337e-02]\n",
      " [9.93467033e-01 3.15953093e-06]\n",
      " [1.00313846e-05 9.91105139e-01]\n",
      " [1.24554820e-02 2.95013428e-01]\n",
      " [1.00057363e-03 9.02945340e-01]\n",
      " [8.66522896e-05 8.96724045e-01]\n",
      " [7.62889802e-01 1.54930474e-02]\n",
      " [5.24826467e-01 4.89288084e-02]\n",
      " [9.61664227e-06 9.77571428e-01]\n",
      " [9.99889970e-01 4.59325422e-09]\n",
      " [2.23407915e-05 9.91207719e-01]\n",
      " [1.68360875e-05 9.90634620e-01]\n",
      " [3.14601813e-03 6.52863324e-01]\n",
      " [9.98677075e-01 1.35154824e-06]\n",
      " [8.73529792e-01 1.47047773e-04]\n",
      " [5.68001473e-04 8.93587947e-01]\n",
      " [9.86817420e-01 5.53788459e-06]\n",
      " [6.78891875e-03 8.21716189e-01]\n",
      " [5.06963964e-08 9.98648226e-01]\n",
      " [3.57070286e-03 9.11852598e-01]\n",
      " [1.13683147e-02 7.92181969e-01]\n",
      " [9.99738872e-01 4.83980074e-08]\n",
      " [9.14594915e-04 2.39006773e-01]\n",
      " [1.78471691e-06 9.88885045e-01]\n",
      " [1.45703807e-01 4.95208427e-03]\n",
      " [9.72705901e-01 2.89572417e-05]\n",
      " [7.53262043e-01 2.62862250e-05]\n",
      " [9.01326716e-01 1.65630976e-04]\n",
      " [8.44025493e-01 5.34542836e-03]\n",
      " [1.78361438e-06 9.97089803e-01]\n",
      " [9.46478128e-01 1.62629833e-04]\n",
      " [9.23010428e-03 5.24178386e-01]\n",
      " [2.07661837e-03 7.82669365e-01]\n",
      " [2.90991680e-04 9.47339594e-01]\n",
      " [9.78298485e-01 2.63494934e-04]\n",
      " [8.75996828e-01 4.60975949e-04]\n",
      " [5.72559424e-04 7.94612050e-01]\n",
      " [2.36389242e-04 8.93629014e-01]\n",
      " [4.29314941e-01 5.10198250e-02]\n",
      " [9.60474133e-01 1.02344929e-04]\n",
      " [9.98147845e-01 1.24187795e-06]\n",
      " [1.06717774e-03 9.50377107e-01]\n",
      " [9.99120414e-01 4.13041448e-07]\n",
      " [6.44826423e-03 8.80439162e-01]\n",
      " [3.53974029e-02 8.05739164e-02]\n",
      " [1.47425104e-03 8.90067041e-01]\n",
      " [1.62754592e-03 8.56621802e-01]\n",
      " [5.83256006e-01 8.40801629e-04]\n",
      " [2.58376151e-02 3.72594267e-01]\n",
      " [1.90140287e-04 9.47895169e-01]\n",
      " [6.18401349e-01 1.05441420e-03]\n",
      " [1.26560961e-04 9.79726136e-01]\n",
      " [6.05927853e-05 8.12421203e-01]\n",
      " [2.90721044e-04 9.50782895e-01]\n",
      " [8.95724833e-01 4.53538349e-04]\n",
      " [1.21813186e-03 9.07149196e-01]\n",
      " [9.96119022e-01 7.22317145e-06]\n",
      " [4.44982521e-04 9.14272785e-01]\n",
      " [9.93605614e-01 9.91565139e-06]\n",
      " [9.64348167e-02 4.75731753e-02]\n",
      " [9.86183643e-01 6.33023810e-05]\n",
      " [7.98950493e-01 2.99635087e-03]\n",
      " [9.87302184e-01 8.25217285e-06]\n",
      " [1.34985313e-01 1.11850269e-01]\n",
      " [9.89539444e-01 2.70625387e-05]\n",
      " [9.48837519e-01 1.72300934e-05]\n",
      " [3.34199704e-02 3.22655320e-01]\n",
      " [5.84032619e-03 2.73369074e-01]\n",
      " [4.58089235e-05 9.76366282e-01]\n",
      " [2.41539121e-04 9.46836531e-01]\n",
      " [3.14406607e-05 9.83765900e-01]\n",
      " [4.50252416e-03 2.16848314e-01]\n",
      " [2.99695641e-01 1.46127865e-02]\n",
      " [9.98656988e-01 1.36700930e-06]\n",
      " [6.04652226e-01 7.29631633e-03]\n",
      " [9.59540367e-01 1.86229678e-04]\n",
      " [9.81599867e-01 1.03508763e-04]\n",
      " [2.63895933e-02 5.92859276e-02]\n",
      " [8.05364728e-01 5.92293870e-03]\n",
      " [1.07019441e-04 9.56027389e-01]\n",
      " [9.95643020e-01 3.26583972e-06]\n",
      " [4.02529423e-07 9.95025933e-01]\n",
      " [5.02539615e-06 9.94444907e-01]\n",
      " [7.90014505e-01 2.49488094e-05]\n",
      " [1.45879795e-03 6.65735066e-01]\n",
      " [2.84177181e-03 9.10104215e-01]\n",
      " [1.68603137e-01 2.19383892e-02]\n",
      " [1.24850846e-03 4.06450421e-01]\n",
      " [3.79005156e-04 9.67722178e-01]\n",
      " [3.34322598e-04 5.10804176e-01]\n",
      " [8.08995605e-01 1.88738368e-02]\n",
      " [4.45854539e-06 9.93593156e-01]\n",
      " [3.87870789e-01 6.22556265e-03]\n",
      " [9.99963760e-01 2.60892252e-10]\n",
      " [9.48027372e-01 9.97750176e-05]\n",
      " [7.55919218e-02 3.26003164e-01]\n",
      " [2.43130699e-02 1.24233179e-02]\n",
      " [6.45541400e-03 8.88169110e-01]\n",
      " [6.17849524e-04 2.87313521e-01]\n",
      " [9.95074689e-01 7.62500440e-06]\n",
      " [5.55340247e-03 8.78583312e-01]\n",
      " [9.99813497e-01 2.63390483e-08]\n",
      " [9.98190701e-01 4.99148484e-07]\n",
      " [2.03377940e-03 7.91381001e-01]\n",
      " [9.90932345e-01 8.03395324e-06]\n",
      " [8.47961009e-01 7.61563284e-03]\n",
      " [2.90168886e-04 9.65958059e-01]\n",
      " [9.92745578e-01 2.24528831e-05]\n",
      " [5.70971933e-05 9.85976100e-01]\n",
      " [6.85293751e-04 9.48348820e-01]\n",
      " [4.34841386e-05 9.60738361e-01]\n",
      " [9.82853174e-02 8.47287476e-02]\n",
      " [8.72010708e-01 1.72530767e-04]\n",
      " [8.32632303e-01 1.51609024e-03]\n",
      " [3.56897712e-03 3.95975083e-01]\n",
      " [7.93196136e-07 9.95035827e-01]\n",
      " [8.12882781e-01 2.57793609e-02]\n",
      " [9.96775568e-01 2.33608807e-06]\n",
      " [3.73504416e-04 9.45890784e-01]\n",
      " [9.84407234e-06 9.39555109e-01]\n",
      " [2.36254600e-05 9.88878489e-01]\n",
      " [6.03997052e-01 9.65748564e-04]\n",
      " [9.60274847e-05 6.87182605e-01]\n",
      " [4.65495557e-01 1.56345777e-02]\n",
      " [3.48430622e-05 9.76217866e-01]\n",
      " [9.87743497e-01 3.16410336e-07]\n",
      " [9.96151388e-01 5.22823166e-06]\n",
      " [3.32645650e-05 9.26128983e-01]\n",
      " [9.87971475e-07 9.96169031e-01]\n",
      " [1.56864597e-04 9.37036037e-01]\n",
      " [3.16514909e-01 9.48317815e-03]\n",
      " [9.45484698e-01 1.12701242e-03]\n",
      " [3.92445445e-01 4.76294430e-03]\n",
      " [4.52173222e-03 8.39222729e-01]\n",
      " [5.12804426e-02 5.68297923e-01]\n",
      " [6.34382232e-05 9.60166276e-01]\n",
      " [7.01211263e-07 9.96533871e-01]\n",
      " [9.98557389e-01 1.19976596e-06]\n",
      " [8.66565943e-01 1.39259163e-03]\n",
      " [3.49865388e-03 4.48631555e-01]\n",
      " [9.97335017e-01 2.44705257e-06]\n",
      " [1.18978431e-04 8.91244471e-01]\n",
      " [4.69631428e-04 5.34747303e-01]\n",
      " [9.63346660e-01 5.79183034e-05]\n",
      " [1.32645795e-03 5.03336072e-01]\n",
      " [1.38607407e-02 1.04823545e-01]\n",
      " [3.95484143e-08 9.99267995e-01]\n",
      " [4.26501129e-03 5.37881970e-01]\n",
      " [1.47197291e-03 8.43422294e-01]\n",
      " [9.98936236e-01 1.86507734e-07]\n",
      " [8.01652763e-03 5.21321237e-01]\n",
      " [9.44032148e-02 9.78910644e-03]\n",
      " [4.85158563e-01 2.40474311e-03]\n",
      " [9.23079014e-01 1.20595127e-04]\n",
      " [9.43399787e-01 1.82505068e-03]\n",
      " [8.74756500e-02 1.17921196e-01]\n",
      " [7.35130489e-01 3.64416657e-04]\n",
      " [2.22621299e-03 3.23700607e-01]\n",
      " [9.15166177e-03 8.53774726e-01]\n",
      " [2.87880868e-01 2.14199536e-03]\n",
      " [3.41822603e-03 8.56669068e-01]\n",
      " [3.34302336e-02 2.31326833e-01]\n",
      " [9.06747520e-01 5.30847767e-03]\n",
      " [1.66506761e-05 9.89060283e-01]\n",
      " [2.95772916e-04 9.70229506e-01]\n",
      " [4.46343809e-01 1.20151043e-02]\n",
      " [9.95595992e-01 1.19920321e-06]\n",
      " [9.74779785e-01 2.57249398e-04]\n",
      " [9.00487363e-01 2.97969236e-04]\n",
      " [2.13376689e-03 6.16064489e-01]\n",
      " [3.83675355e-03 6.96350634e-01]\n",
      " [8.55465055e-01 1.06049748e-03]\n",
      " [2.74940743e-03 4.91248399e-01]\n",
      " [2.64421642e-01 2.11683661e-02]\n",
      " [4.01296504e-02 1.25489071e-01]\n",
      " [3.21302650e-04 7.94565499e-01]\n",
      " [7.37305164e-01 5.97573370e-02]\n",
      " [9.97034073e-01 1.38807582e-06]\n",
      " [9.94793475e-01 1.05223753e-05]\n",
      " [9.74139571e-01 1.66210637e-04]\n",
      " [2.67944906e-05 9.88760531e-01]\n",
      " [8.26588273e-02 6.00863099e-01]\n",
      " [9.98991668e-01 5.89284241e-07]\n",
      " [1.49167711e-02 1.88180506e-01]\n",
      " [1.01085207e-04 9.34175551e-01]\n",
      " [4.87283897e-03 4.70082551e-01]\n",
      " [9.98665094e-01 5.51691926e-07]\n",
      " [3.63344669e-01 1.08220363e-02]\n",
      " [1.39722970e-05 9.91953433e-01]\n",
      " [9.66177904e-04 8.07273865e-01]\n",
      " [1.44631760e-02 3.00290361e-02]\n",
      " [2.75260266e-02 1.16818354e-01]\n",
      " [3.98108423e-05 8.71842504e-01]\n",
      " [9.93299842e-01 2.04068474e-05]\n",
      " [7.91486260e-03 1.07604116e-01]\n",
      " [3.74835399e-05 9.80796218e-01]\n",
      " [4.55400884e-01 1.23653170e-02]\n",
      " [1.79165532e-03 9.01369989e-01]\n",
      " [5.13012558e-02 1.69451833e-01]\n",
      " [2.38167122e-01 1.38987929e-01]\n",
      " [9.71179783e-01 9.23269617e-05]\n",
      " [1.53441101e-06 9.96016800e-01]\n",
      " [9.43417490e-01 3.53292737e-04]\n",
      " [1.59649213e-03 7.43265510e-01]\n",
      " [6.18727528e-04 9.17712390e-01]\n",
      " [9.81332541e-01 1.65584515e-05]\n",
      " [1.19690681e-02 2.04215571e-01]\n",
      " [2.84190919e-05 9.46211398e-01]\n",
      " [9.98810530e-01 2.48450931e-07]\n",
      " [3.71776696e-04 9.66306806e-01]\n",
      " [2.67049391e-03 3.78305078e-01]\n",
      " [9.99256670e-01 4.43434004e-07]\n",
      " [8.32174301e-01 1.92108769e-02]\n",
      " [9.62494254e-01 2.15853492e-04]\n",
      " [6.74666286e-01 1.61244813e-02]\n",
      " [4.48486581e-02 1.12388290e-01]\n",
      " [1.82431433e-02 4.18366164e-01]\n",
      " [1.85097597e-05 9.88709986e-01]\n",
      " [2.95061764e-04 9.63725626e-01]\n",
      " [1.60840063e-05 9.75886166e-01]\n",
      " [6.92249974e-04 8.37682784e-01]\n",
      " [1.38626001e-05 9.90774572e-01]\n",
      " [2.05406046e-04 9.37512755e-01]\n",
      " [9.97196674e-01 2.05870265e-06]\n",
      " [7.95067847e-01 1.22531466e-02]\n",
      " [1.18702399e-02 6.68460190e-01]]\n",
      "accuracy_score  0.665625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_pred = model.predict(x_val,batch_size=200)\n",
    "print(y_val)\n",
    "print(model_pred)\n",
    "print(\"accuracy_score \",accuracy_score(y_val,model_pred > 0.5, normalize=True, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "python_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

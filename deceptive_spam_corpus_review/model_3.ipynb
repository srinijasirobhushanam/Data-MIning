{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of truthfuls reviews  800\n",
      "Number of deceptives reviews  800\n"
     ]
    }
   ],
   "source": [
    "truthful_pos = 'op_spam_v1.4/positive_polarity/truthful_from_TripAdvisor/'\n",
    "truthful_neg = 'op_spam_v1.4/negative_polarity/truthful_from_Web/'\n",
    "\n",
    "deceptive_pos = 'op_spam_v1.4/positive_polarity/deceptive_from_MTurk/'\n",
    "deceptive_neg = 'op_spam_v1.4/negative_polarity/deceptive_from_MTurk/'\n",
    "\n",
    "truthful_reviews_link = []\n",
    "for fold in os.listdir(truthful_pos):\n",
    "    foldLink = os.path.join(truthful_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(truthful_neg):\n",
    "    foldLink = os.path.join(truthful_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "deceptive_reviews_link = []\n",
    "\n",
    "for fold in os.listdir(deceptive_pos):\n",
    "    foldLink = os.path.join(deceptive_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(deceptive_neg):\n",
    "    foldLink = os.path.join(deceptive_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "        \n",
    "print('Number of truthfuls reviews ', len(truthful_reviews_link))\n",
    "print('Number of deceptives reviews ', len(deceptive_reviews_link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is  1600\n",
      "The total number of words in the files is  253157\n",
      "Vocabulary size is  9687\n",
      "The average number of words in the files is 158.223125\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def handleFile(filePath):\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        file_voc = []\n",
    "        file_numWords = 0\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            words = cleanedLine.split(' ')\n",
    "            file_numWords = file_numWords + len(words)\n",
    "            file_voc.extend(words)\n",
    "    return file_voc, file_numWords\n",
    "\n",
    "\n",
    "allFilesLinks = truthful_reviews_link + deceptive_reviews_link\n",
    "vocabulary = []\n",
    "numWords = []\n",
    "for fileLink in allFilesLinks:\n",
    "    file_voc, file_numWords = handleFile(fileLink)\n",
    "    vocabulary.extend(file_voc)\n",
    "    numWords.append(file_numWords)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print('The total number of files is ', len(numWords))\n",
    "print('The total number of words in the files is ', sum(numWords))\n",
    "print('Vocabulary size is ', len(vocabulary))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "def convertFileToArray(filePath):\n",
    "    s = \"\"\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            s += cleanedLine\n",
    "    return s\n",
    "\n",
    "totalFiles = len(truthful_reviews_link) + len(deceptive_reviews_link)\n",
    "idsMatrix = np.ndarray(shape=(totalFiles, MAX_SEQ_LENGTH), dtype='int32')\n",
    "#dataMatrix = np.ndarray(shape=(totalFiles,1),dtype='object')\n",
    "dataMatrix = []\n",
    "#labels = np.ndarray(shape=(totalFiles, 2), dtype='int32')\n",
    "labelsMatrix = []\n",
    "counter = 0\n",
    "\n",
    "for filePath in truthful_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(1)\n",
    "  \n",
    "\n",
    "for filePath in deceptive_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(0)\n",
    "\n",
    "dict_reviewLabels = {'review': dataMatrix,'labels': labelsMatrix}\n",
    "df_reviewLabels = pd.DataFrame(dict_reviewLabels)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "macronum=sorted(set(df_reviewLabels['labels']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "df_reviewLabels.iloc[:,0]=df_reviewLabels.iloc[:,0].apply(fun)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "print(df_reviewLabels.shape)\n",
    "# a list contains each review as a list \n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "\n",
    "for i in range(len(df_reviewLabels)):\n",
    "    balanced_texts.append(df_reviewLabels.iloc[i,1])\n",
    "    balanced_labels.append(df_reviewLabels.iloc[i,0])\n",
    " \n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \")#20000\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "x = pad_sequences(sequences, maxlen=200)#300\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(balanced_labels))\n",
    "#y = df_reviewLabels['labels'].values\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, GlobalMaxPooling1D, Conv1D, Dropout, MaxPooling1D, Dense, Embedding, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "# Build embedding layers with weights initialized from each model\n",
    "googlenews_w2v_size = 300\n",
    "googlenews_w2v_matrix = np.zeros((len(word_index) + 1, googlenews_w2v_size))\n",
    "for word,i in word_index.items():\n",
    "    try:\n",
    "        if word in w2v_model.vocab:\n",
    "            googlenews_w2v_matrix[i] = w2v_model[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "googlenews_w2v_emb = Embedding(len(word_index)+1,\n",
    "\n",
    "                            googlenews_w2v_size,\n",
    "\n",
    "                            weights=[googlenews_w2v_matrix],\n",
    "\n",
    "                            input_length=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * x.shape[0])\n",
    "\n",
    "x_train = x[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = x[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20)                19260     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,924,202\n",
      "Trainable params: 2,924,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6961 - acc: 0.4935 - val_loss: 0.6877 - val_acc: 0.5547\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6851 - acc: 0.5846 - val_loss: 0.6833 - val_acc: 0.5645\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6740 - acc: 0.6497 - val_loss: 0.6790 - val_acc: 0.5937\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6624 - acc: 0.6706 - val_loss: 0.6746 - val_acc: 0.6152\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6453 - acc: 0.7096 - val_loss: 0.6696 - val_acc: 0.6035\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6354 - acc: 0.7227 - val_loss: 0.6646 - val_acc: 0.6074\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6207 - acc: 0.7383 - val_loss: 0.6595 - val_acc: 0.6074\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6002 - acc: 0.7435 - val_loss: 0.6540 - val_acc: 0.6113\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5791 - acc: 0.7565 - val_loss: 0.6481 - val_acc: 0.6133\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5588 - acc: 0.7656 - val_loss: 0.6418 - val_acc: 0.6191\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5291 - acc: 0.7773 - val_loss: 0.6353 - val_acc: 0.6309\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.5011 - acc: 0.7995 - val_loss: 0.6280 - val_acc: 0.6367\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.4695 - acc: 0.8216 - val_loss: 0.6210 - val_acc: 0.6523\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.4259 - acc: 0.8607 - val_loss: 0.6098 - val_acc: 0.6621\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.3858 - acc: 0.8555 - val_loss: 0.6057 - val_acc: 0.6699\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.3409 - acc: 0.8945 - val_loss: 0.6026 - val_acc: 0.6777\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.2928 - acc: 0.9115 - val_loss: 0.6093 - val_acc: 0.6953\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.2456 - acc: 0.9310 - val_loss: 0.6281 - val_acc: 0.7090\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.2088 - acc: 0.9336 - val_loss: 0.6632 - val_acc: 0.7090\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1765 - acc: 0.9531 - val_loss: 0.6848 - val_acc: 0.7129\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1508 - acc: 0.9648 - val_loss: 0.6771 - val_acc: 0.7168\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1258 - acc: 0.9701 - val_loss: 0.7001 - val_acc: 0.7148\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.1118 - acc: 0.9766 - val_loss: 0.6969 - val_acc: 0.7285\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0956 - acc: 0.9831 - val_loss: 0.7223 - val_acc: 0.7246\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0936 - acc: 0.9831 - val_loss: 0.7440 - val_acc: 0.7246\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0860 - acc: 0.9831 - val_loss: 0.7475 - val_acc: 0.7324\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0807 - acc: 0.9857 - val_loss: 0.7762 - val_acc: 0.7246\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0767 - acc: 0.9870 - val_loss: 0.7764 - val_acc: 0.7227\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0689 - acc: 0.9870 - val_loss: 0.7877 - val_acc: 0.7266\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0671 - acc: 0.9870 - val_loss: 0.8188 - val_acc: 0.7109\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0628 - acc: 0.9896 - val_loss: 0.8261 - val_acc: 0.7168\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0542 - acc: 0.9909 - val_loss: 0.8480 - val_acc: 0.7168\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0525 - acc: 0.9896 - val_loss: 1.0060 - val_acc: 0.6836\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0678 - acc: 0.9870 - val_loss: 0.8174 - val_acc: 0.7051\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0637 - acc: 0.9844 - val_loss: 0.8296 - val_acc: 0.7109\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0620 - acc: 0.9883 - val_loss: 0.9298 - val_acc: 0.7168\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0435 - acc: 0.9935 - val_loss: 0.8651 - val_acc: 0.7168\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0422 - acc: 0.9935 - val_loss: 0.8872 - val_acc: 0.7187\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0356 - acc: 0.9935 - val_loss: 0.9431 - val_acc: 0.7168\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0354 - acc: 0.9948 - val_loss: 0.9630 - val_acc: 0.7148\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0291 - acc: 0.9961 - val_loss: 0.9149 - val_acc: 0.7168\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0297 - acc: 0.9961 - val_loss: 0.9226 - val_acc: 0.7070\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0258 - acc: 0.9961 - val_loss: 0.9438 - val_acc: 0.7070\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0252 - acc: 0.9961 - val_loss: 0.9671 - val_acc: 0.7070\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.0301 - acc: 0.9948 - val_loss: 0.9899 - val_acc: 0.7109\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "#GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(GRU(units=20,activation='tanh',recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,batch_size=200,epochs=45,validation_split=0.4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8820\n",
      "Testing Accuracy:  0.7031\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 82        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,956,342\n",
      "Trainable params: 2,956,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 5s 6ms/step - loss: 0.6888 - acc: 0.5859 - val_loss: 0.6903 - val_acc: 0.5352\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6803 - acc: 0.6536 - val_loss: 0.6871 - val_acc: 0.5723\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6718 - acc: 0.7266 - val_loss: 0.6836 - val_acc: 0.5957\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6606 - acc: 0.7565 - val_loss: 0.6793 - val_acc: 0.6074\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6478 - acc: 0.7839 - val_loss: 0.6729 - val_acc: 0.6250\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.6250 - acc: 0.8333 - val_loss: 0.6636 - val_acc: 0.6426\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.5981 - acc: 0.8581 - val_loss: 0.6499 - val_acc: 0.6660\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.5544 - acc: 0.8737 - val_loss: 0.6295 - val_acc: 0.6953\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.4939 - acc: 0.9167 - val_loss: 0.5891 - val_acc: 0.7344\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.4057 - acc: 0.9544 - val_loss: 0.5486 - val_acc: 0.7500\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.3538 - acc: 0.9323 - val_loss: 0.5425 - val_acc: 0.7422\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.3187 - acc: 0.9427 - val_loss: 0.5280 - val_acc: 0.7598\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.2650 - acc: 0.9792 - val_loss: 0.5320 - val_acc: 0.7480\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.2284 - acc: 0.9805 - val_loss: 0.5183 - val_acc: 0.7520\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1989 - acc: 0.9844 - val_loss: 0.5189 - val_acc: 0.7559\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1717 - acc: 0.9844 - val_loss: 0.5277 - val_acc: 0.7559\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1434 - acc: 0.9844 - val_loss: 0.5366 - val_acc: 0.7539\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1274 - acc: 0.9883 - val_loss: 0.5490 - val_acc: 0.7617\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.1129 - acc: 0.9896 - val_loss: 0.5615 - val_acc: 0.7559\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0922 - acc: 0.9935 - val_loss: 0.5955 - val_acc: 0.7422\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0810 - acc: 0.9961 - val_loss: 0.5984 - val_acc: 0.7520\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0676 - acc: 0.9974 - val_loss: 0.6129 - val_acc: 0.7598\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0610 - acc: 0.9961 - val_loss: 0.6348 - val_acc: 0.7539\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0550 - acc: 0.9974 - val_loss: 0.6505 - val_acc: 0.7520\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0462 - acc: 0.9974 - val_loss: 0.6710 - val_acc: 0.7559\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0430 - acc: 0.9974 - val_loss: 0.6899 - val_acc: 0.7539\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0416 - acc: 0.9974 - val_loss: 0.7136 - val_acc: 0.7539\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0410 - acc: 0.9974 - val_loss: 0.7292 - val_acc: 0.7539\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0346 - acc: 0.9974 - val_loss: 0.7480 - val_acc: 0.7520\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0316 - acc: 0.9974 - val_loss: 0.7773 - val_acc: 0.7402\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0299 - acc: 0.9974 - val_loss: 0.7854 - val_acc: 0.7402\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0293 - acc: 0.9974 - val_loss: 0.8039 - val_acc: 0.7305\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0276 - acc: 0.9974 - val_loss: 0.8126 - val_acc: 0.7305\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0253 - acc: 0.9974 - val_loss: 0.8181 - val_acc: 0.7461\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0251 - acc: 0.9987 - val_loss: 0.8283 - val_acc: 0.7441\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0224 - acc: 0.9987 - val_loss: 0.8692 - val_acc: 0.7266\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0257 - acc: 0.9948 - val_loss: 0.8083 - val_acc: 0.7520\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0249 - acc: 0.9974 - val_loss: 0.8830 - val_acc: 0.7305\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.8489 - val_acc: 0.7324\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.8387 - val_acc: 0.7402\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.8672 - val_acc: 0.7441\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.8848 - val_acc: 0.7461\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.8805 - val_acc: 0.7441\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.8809 - val_acc: 0.7422\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 4s 5ms/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.8814 - val_acc: 0.7402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15245ed58128>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM, Bidirectional\n",
    "#Bi directional LSTM\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Bidirectional(LSTM(units=20,activation='tanh',recurrent_activation='hard_sigmoid')))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, nb_epoch=45,validation_split=0.4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8961\n",
      "Testing Accuracy:  0.7156\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20)                25680     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,930,622\n",
      "Trainable params: 2,930,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:15: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 3s 4ms/step - loss: 0.6924 - acc: 0.4987 - val_loss: 0.6855 - val_acc: 0.5781\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6734 - acc: 0.6484 - val_loss: 0.6779 - val_acc: 0.6035\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6502 - acc: 0.7435 - val_loss: 0.6688 - val_acc: 0.6348\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6208 - acc: 0.8164 - val_loss: 0.6568 - val_acc: 0.6426\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.5857 - acc: 0.8451 - val_loss: 0.6401 - val_acc: 0.6641\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.5332 - acc: 0.8841 - val_loss: 0.6174 - val_acc: 0.6797\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.4659 - acc: 0.8919 - val_loss: 0.5854 - val_acc: 0.7070\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.3664 - acc: 0.9440 - val_loss: 0.5460 - val_acc: 0.7383\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.2472 - acc: 0.9805 - val_loss: 0.5359 - val_acc: 0.7480\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1978 - acc: 0.9674 - val_loss: 0.6072 - val_acc: 0.7051\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1797 - acc: 0.9596 - val_loss: 0.5769 - val_acc: 0.7402\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1419 - acc: 0.9805 - val_loss: 0.5528 - val_acc: 0.7578\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1174 - acc: 0.9922 - val_loss: 0.5631 - val_acc: 0.7539\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.1168 - acc: 0.9922 - val_loss: 0.5820 - val_acc: 0.7461\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0943 - acc: 0.9948 - val_loss: 0.6271 - val_acc: 0.7422\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0716 - acc: 0.9961 - val_loss: 0.6318 - val_acc: 0.7480\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0613 - acc: 0.9974 - val_loss: 0.6645 - val_acc: 0.7461\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0601 - acc: 0.9935 - val_loss: 0.7149 - val_acc: 0.7402\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0547 - acc: 0.9948 - val_loss: 0.6865 - val_acc: 0.7539\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0501 - acc: 0.9948 - val_loss: 0.7096 - val_acc: 0.7598\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0460 - acc: 0.9974 - val_loss: 0.7381 - val_acc: 0.7422\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0369 - acc: 0.9974 - val_loss: 0.7692 - val_acc: 0.7402\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0355 - acc: 0.9987 - val_loss: 0.7667 - val_acc: 0.7500\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0320 - acc: 0.9987 - val_loss: 0.7542 - val_acc: 0.7539\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.7623 - val_acc: 0.7578\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.8110 - val_acc: 0.7422\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0251 - acc: 0.9974 - val_loss: 0.8489 - val_acc: 0.7363\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.8384 - val_acc: 0.7422\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0270 - acc: 0.9974 - val_loss: 0.8418 - val_acc: 0.7402\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0235 - acc: 0.9974 - val_loss: 0.8167 - val_acc: 0.7520\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.8504 - val_acc: 0.7500\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.8894 - val_acc: 0.7344\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0206 - acc: 0.9987 - val_loss: 0.8911 - val_acc: 0.7441\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.8556 - val_acc: 0.7500\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.8693 - val_acc: 0.7480\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.8877 - val_acc: 0.7441\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.8969 - val_acc: 0.7441\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.9107 - val_acc: 0.7559\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.9545 - val_acc: 0.7480\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.9898 - val_acc: 0.7383\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 1.0109 - val_acc: 0.7344\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 1.0168 - val_acc: 0.7383\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0150 - acc: 0.9987 - val_loss: 1.0305 - val_acc: 0.7305\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0284 - acc: 0.9961 - val_loss: 0.9233 - val_acc: 0.7578\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.9677 - val_acc: 0.7422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15245cdd84e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units=20,activation='tanh',recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, nb_epoch=45,validation_split=0.4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8969\n",
      "Testing Accuracy:  0.7250\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(strides=1, activation=\"relu\", padding=\"same\", kernel_size=3, filters=150)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 150)          135150    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 150)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 20)                13680     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,053,772\n",
      "Trainable params: 3,053,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 3s 4ms/step - loss: 0.6868 - acc: 0.5651 - val_loss: 0.6721 - val_acc: 0.6367\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.6216 - acc: 0.8359 - val_loss: 0.6472 - val_acc: 0.6895\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.5430 - acc: 0.8724 - val_loss: 0.6148 - val_acc: 0.6836\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.4326 - acc: 0.8984 - val_loss: 0.5742 - val_acc: 0.7090\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.2915 - acc: 0.9466 - val_loss: 0.5272 - val_acc: 0.7461\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.1843 - acc: 0.9805 - val_loss: 0.4886 - val_acc: 0.7793\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.1339 - acc: 0.9792 - val_loss: 0.4894 - val_acc: 0.7930\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.1032 - acc: 0.9844 - val_loss: 0.5313 - val_acc: 0.7754\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0787 - acc: 0.9896 - val_loss: 0.5747 - val_acc: 0.7598\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0575 - acc: 0.9948 - val_loss: 0.5966 - val_acc: 0.7578\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0522 - acc: 0.9922 - val_loss: 0.6717 - val_acc: 0.7617\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.6576 - val_acc: 0.7656\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0362 - acc: 0.9987 - val_loss: 0.6671 - val_acc: 0.7676\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0269 - acc: 1.0000 - val_loss: 0.6972 - val_acc: 0.7676\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0219 - acc: 0.9987 - val_loss: 0.7326 - val_acc: 0.7734\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.7443 - val_acc: 0.7734\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.7623 - val_acc: 0.7754\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.7770 - val_acc: 0.7656\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.7901 - val_acc: 0.7695\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.8014 - val_acc: 0.7715\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.8214 - val_acc: 0.7734\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0156 - acc: 0.9987 - val_loss: 0.9109 - val_acc: 0.7578\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0145 - acc: 0.9987 - val_loss: 0.8185 - val_acc: 0.7793\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0177 - acc: 0.9987 - val_loss: 0.8350 - val_acc: 0.7793\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0157 - acc: 0.9987 - val_loss: 0.8454 - val_acc: 0.7715\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.7671 - val_acc: 0.7422\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0221 - acc: 1.0000 - val_loss: 0.8309 - val_acc: 0.7598\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.8209 - val_acc: 0.7559\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.8126 - val_acc: 0.7637\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.8511 - val_acc: 0.7656\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.8738 - val_acc: 0.7695\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.8959 - val_acc: 0.7637\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0160 - acc: 0.9974 - val_loss: 0.8771 - val_acc: 0.7695\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0104 - acc: 0.9987 - val_loss: 0.9319 - val_acc: 0.7637\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0104 - acc: 0.9987 - val_loss: 0.8845 - val_acc: 0.7637\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.8968 - val_acc: 0.7656\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0105 - acc: 0.9987 - val_loss: 0.9378 - val_acc: 0.7695\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 1.0514 - val_acc: 0.7520\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0143 - acc: 0.9974 - val_loss: 1.0254 - val_acc: 0.7461\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0645 - acc: 0.9805 - val_loss: 1.1359 - val_acc: 0.7305\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0408 - acc: 0.9831 - val_loss: 1.0905 - val_acc: 0.6992\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0601 - acc: 0.9792 - val_loss: 0.9016 - val_acc: 0.7363\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0585 - acc: 0.9753 - val_loss: 0.8279 - val_acc: 0.7539\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0267 - acc: 0.9935 - val_loss: 0.9722 - val_acc: 0.7441\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 0.0147 - acc: 0.9974 - val_loss: 0.8858 - val_acc: 0.7754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15245c0e9390>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN + LSTM\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution1D(nb_filter=150,\n",
    "                        filter_length=3,\n",
    "                        border_mode='same',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(units=20,activation='tanh',recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, epochs=45,validation_split=0.4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9102\n",
      "Testing Accuracy:  0.7625\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(strides=1, activation=\"relu\", padding=\"valid\", kernel_size=3, filters=100)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(strides=1, activation=\"relu\", padding=\"valid\", kernel_size=4, filters=100)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(strides=1, activation=\"relu\", padding=\"valid\", kernel_size=5, filters=100)`\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          2904900   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 29500)             360300    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                1888064   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 5,153,394\n",
      "Trainable params: 5,153,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 768 samples, validate on 512 samples\n",
      "Epoch 1/45\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 0.6942 - acc: 0.5143 - val_loss: 0.6923 - val_acc: 0.4883\n",
      "Epoch 2/45\n",
      "768/768 [==============================] - 1s 846us/step - loss: 0.7037 - acc: 0.5560 - val_loss: 0.6929 - val_acc: 0.4941\n",
      "Epoch 3/45\n",
      "768/768 [==============================] - 1s 845us/step - loss: 0.6932 - acc: 0.5977 - val_loss: 0.6931 - val_acc: 0.5684\n",
      "Epoch 4/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.6932 - acc: 0.5482 - val_loss: 0.6931 - val_acc: 0.5117\n",
      "Epoch 5/45\n",
      "768/768 [==============================] - 1s 845us/step - loss: 0.6927 - acc: 0.5404 - val_loss: 0.6931 - val_acc: 0.5117\n",
      "Epoch 6/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.6930 - acc: 0.5221 - val_loss: 0.6931 - val_acc: 0.5117\n",
      "Epoch 7/45\n",
      "768/768 [==============================] - 1s 842us/step - loss: 0.6924 - acc: 0.5208 - val_loss: 0.6931 - val_acc: 0.5117\n",
      "Epoch 8/45\n",
      "768/768 [==============================] - 1s 845us/step - loss: 0.6922 - acc: 0.5391 - val_loss: 0.6931 - val_acc: 0.5117\n",
      "Epoch 9/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.6934 - acc: 0.5104 - val_loss: 0.6930 - val_acc: 0.5117\n",
      "Epoch 10/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.6918 - acc: 0.5065 - val_loss: 0.6928 - val_acc: 0.5117\n",
      "Epoch 11/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.6947 - acc: 0.5143 - val_loss: 0.6924 - val_acc: 0.5117\n",
      "Epoch 12/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.6899 - acc: 0.5391 - val_loss: 0.6906 - val_acc: 0.5117\n",
      "Epoch 13/45\n",
      "768/768 [==============================] - 1s 842us/step - loss: 0.6771 - acc: 0.5404 - val_loss: 0.6714 - val_acc: 0.5117\n",
      "Epoch 14/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.6211 - acc: 0.5156 - val_loss: 0.6122 - val_acc: 0.5215\n",
      "Epoch 15/45\n",
      "768/768 [==============================] - 1s 839us/step - loss: 0.4888 - acc: 0.6159 - val_loss: 0.5810 - val_acc: 0.6914\n",
      "Epoch 16/45\n",
      "768/768 [==============================] - 1s 845us/step - loss: 0.3843 - acc: 0.8789 - val_loss: 0.5655 - val_acc: 0.7773\n",
      "Epoch 17/45\n",
      "768/768 [==============================] - 1s 840us/step - loss: 0.1854 - acc: 0.9596 - val_loss: 0.7119 - val_acc: 0.7715\n",
      "Epoch 18/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.0757 - acc: 0.9766 - val_loss: 0.7352 - val_acc: 0.7949\n",
      "Epoch 19/45\n",
      "768/768 [==============================] - 1s 842us/step - loss: 0.0256 - acc: 0.9935 - val_loss: 0.8123 - val_acc: 0.7832\n",
      "Epoch 20/45\n",
      "768/768 [==============================] - 1s 841us/step - loss: 0.0184 - acc: 0.9948 - val_loss: 0.9089 - val_acc: 0.7812\n",
      "Epoch 21/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 1.1371 - val_acc: 0.7676\n",
      "Epoch 22/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.0106 - acc: 0.9987 - val_loss: 1.0720 - val_acc: 0.7852\n",
      "Epoch 23/45\n",
      "768/768 [==============================] - 1s 842us/step - loss: 0.0085 - acc: 0.9961 - val_loss: 1.1438 - val_acc: 0.7812\n",
      "Epoch 24/45\n",
      "768/768 [==============================] - 1s 839us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 1.2533 - val_acc: 0.7793\n",
      "Epoch 25/45\n",
      "768/768 [==============================] - 1s 855us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 1.3120 - val_acc: 0.7773\n",
      "Epoch 26/45\n",
      "768/768 [==============================] - 1s 846us/step - loss: 0.0042 - acc: 0.9974 - val_loss: 1.2935 - val_acc: 0.7832\n",
      "Epoch 27/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.3356 - val_acc: 0.7852\n",
      "Epoch 28/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 1.3883 - val_acc: 0.7813\n",
      "Epoch 29/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 1.4777 - val_acc: 0.7832\n",
      "Epoch 30/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.5288 - val_acc: 0.7832\n",
      "Epoch 31/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.5307 - val_acc: 0.7832\n",
      "Epoch 32/45\n",
      "768/768 [==============================] - 1s 848us/step - loss: 6.9604e-04 - acc: 1.0000 - val_loss: 1.5127 - val_acc: 0.7891\n",
      "Epoch 33/45\n",
      "768/768 [==============================] - 1s 845us/step - loss: 4.3189e-04 - acc: 1.0000 - val_loss: 1.5220 - val_acc: 0.7930\n",
      "Epoch 34/45\n",
      "768/768 [==============================] - 1s 847us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.5396 - val_acc: 0.7871\n",
      "Epoch 35/45\n",
      "768/768 [==============================] - 1s 841us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.5596 - val_acc: 0.7871\n",
      "Epoch 36/45\n",
      "768/768 [==============================] - 1s 840us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.5794 - val_acc: 0.7930\n",
      "Epoch 37/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.6006 - val_acc: 0.7910\n",
      "Epoch 38/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 5.0813e-04 - acc: 1.0000 - val_loss: 1.6191 - val_acc: 0.7852\n",
      "Epoch 39/45\n",
      "768/768 [==============================] - 1s 849us/step - loss: 7.8266e-04 - acc: 1.0000 - val_loss: 1.6378 - val_acc: 0.7832\n",
      "Epoch 40/45\n",
      "768/768 [==============================] - 1s 842us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 1.6352 - val_acc: 0.7930\n",
      "Epoch 41/45\n",
      "768/768 [==============================] - 1s 841us/step - loss: 9.2657e-04 - acc: 1.0000 - val_loss: 1.6284 - val_acc: 0.7891\n",
      "Epoch 42/45\n",
      "768/768 [==============================] - 1s 844us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.6250 - val_acc: 0.7891\n",
      "Epoch 43/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 7.7392e-04 - acc: 1.0000 - val_loss: 1.6309 - val_acc: 0.7891\n",
      "Epoch 44/45\n",
      "768/768 [==============================] - 1s 842us/step - loss: 9.0281e-04 - acc: 1.0000 - val_loss: 1.6449 - val_acc: 0.7852\n",
      "Epoch 45/45\n",
      "768/768 [==============================] - 1s 843us/step - loss: 7.0313e-04 - acc: 1.0000 - val_loss: 1.6549 - val_acc: 0.7910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x152446b3ada0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################################################\n",
    "# CNN\n",
    "# Based on \"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "# https://github.com/keon/keras-text-classification/blob/master/train.py\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import SGD\n",
    "filter_sizes = (3,4,5)\n",
    "num_filters = 100\n",
    "graph_in = Input(shape=(200, googlenews_w2v_size))\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Convolution1D(nb_filter=100,\n",
    "                         filter_length=fsz,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    pool = MaxPooling1D(pool_length=2)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    convs.append(flatten)\n",
    "\n",
    "if len(filter_sizes) > 1:\n",
    "    out = concatenate(convs)\n",
    "    #out = Merge(mode='concat')(convs)\n",
    "else:\n",
    "    out = convs[0]\n",
    "\n",
    "graph = Model(input=graph_in, output=out)\n",
    "model = Sequential()\n",
    "model.add(googlenews_w2v_emb)\n",
    "model.add(Dropout(0.25, input_shape=(200, googlenews_w2v_size)))\n",
    "model.add(graph)\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(macronum)))\n",
    "model.add(Activation('sigmoid'))\n",
    "opt = SGD(lr=0.01, momentum=0.80, decay=1e-6, nesterov=True)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "opt = optimizers.adam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=200, epochs=45,validation_split=0.4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9164\n",
      "Testing Accuracy:  0.7812\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "python_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

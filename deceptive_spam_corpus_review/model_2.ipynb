{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of truthfuls reviews  800\n",
      "Number of deceptives reviews  800\n"
     ]
    }
   ],
   "source": [
    "truthful_pos = 'op_spam_v1.4/positive_polarity/truthful_from_TripAdvisor/'\n",
    "truthful_neg = 'op_spam_v1.4/negative_polarity/truthful_from_Web/'\n",
    "\n",
    "deceptive_pos = 'op_spam_v1.4/positive_polarity/deceptive_from_MTurk/'\n",
    "deceptive_neg = 'op_spam_v1.4/negative_polarity/deceptive_from_MTurk/'\n",
    "\n",
    "truthful_reviews_link = []\n",
    "for fold in os.listdir(truthful_pos):\n",
    "    foldLink = os.path.join(truthful_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(truthful_neg):\n",
    "    foldLink = os.path.join(truthful_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            truthful_reviews_link.append(fileLink)\n",
    "\n",
    "deceptive_reviews_link = []\n",
    "\n",
    "for fold in os.listdir(deceptive_pos):\n",
    "    foldLink = os.path.join(deceptive_pos, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "\n",
    "for fold in os.listdir(deceptive_neg):\n",
    "    foldLink = os.path.join(deceptive_neg, fold)\n",
    "    if os.path.isdir(foldLink):\n",
    "        for f in os.listdir(foldLink):\n",
    "            fileLink = os.path.join(foldLink, f)\n",
    "            deceptive_reviews_link.append(fileLink)\n",
    "        \n",
    "print('Number of truthfuls reviews ', len(truthful_reviews_link))\n",
    "print('Number of deceptives reviews ', len(deceptive_reviews_link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is  1600\n",
      "The total number of words in the files is  253157\n",
      "Vocabulary size is  9687\n",
      "The average number of words in the files is 158.223125\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def handleFile(filePath):\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        file_voc = []\n",
    "        file_numWords = 0\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            words = cleanedLine.split(' ')\n",
    "            file_numWords = file_numWords + len(words)\n",
    "            file_voc.extend(words)\n",
    "    return file_voc, file_numWords\n",
    "\n",
    "\n",
    "allFilesLinks = truthful_reviews_link + deceptive_reviews_link\n",
    "vocabulary = []\n",
    "numWords = []\n",
    "for fileLink in allFilesLinks:\n",
    "    file_voc, file_numWords = handleFile(fileLink)\n",
    "    vocabulary.extend(file_voc)\n",
    "    numWords.append(file_numWords)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print('The total number of files is ', len(numWords))\n",
    "print('The total number of words in the files is ', sum(numWords))\n",
    "print('Vocabulary size is ', len(vocabulary))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Visualize the data in histogram format\"\"\"\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "#plt.show()\n",
    "plt.savefig(\"Sequence_Length vs Frequency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "def convertFileToIndexArray(filePath):\n",
    "    doc = np.zeros(MAX_SEQ_LENGTH, dtype='int32')\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        indexCounter = 0\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            words = cleanedLine.split(' ')\n",
    "            for word in words:\n",
    "                doc[indexCounter] = vocabulary.index(word)\n",
    "                indexCounter = indexCounter + 1\n",
    "                if (indexCounter >= MAX_SEQ_LENGTH):\n",
    "                    break\n",
    "            if (indexCounter >= MAX_SEQ_LENGTH):\n",
    "                break\n",
    "    return doc\n",
    "\n",
    "def convertFileToArray(filePath):\n",
    "    s = \"\"\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            s += cleanedLine\n",
    "    return s\n",
    "\n",
    "totalFiles = len(truthful_reviews_link) + len(deceptive_reviews_link)\n",
    "idsMatrix = np.ndarray(shape=(totalFiles, MAX_SEQ_LENGTH), dtype='int32')\n",
    "#dataMatrix = np.ndarray(shape=(totalFiles,1),dtype='object')\n",
    "dataMatrix = []\n",
    "labels = np.ndarray(shape=(totalFiles, 2), dtype='int32')\n",
    "labelsMatrix = []\n",
    "counter = 0\n",
    "'''\n",
    "for filePath in truthful_reviews_link:\n",
    "    idsMatrix[counter] = convertFileToIndexArray(filePath)\n",
    "    counter = counter + 1\n",
    "\n",
    "for filePath in deceptive_reviews_link:\n",
    "    idsMatrix[counter] = convertFileToIndexArray(filePath)\n",
    "    counter = counter + 1\n",
    "'''\n",
    "for filePath in truthful_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(1)\n",
    "    #dataMatrix[counter] = convertFileToArray(filePath)\n",
    "    #counter = counter + 1\n",
    "\n",
    "for filePath in deceptive_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(0)\n",
    "    #dataMatrix[counter] = convertFileToArray(filePath)\n",
    "    #counter = counter + 1\n",
    "\n",
    "#labels[0:len(truthful_reviews_link)] = np.array([1, 0])\n",
    "#labels[len(truthful_reviews_link):totalFiles] = np.array([0, 1])\n",
    "\n",
    "#print('The shape of the data matrix is ', dataMatrix.shape)\n",
    "#print('The shape of the labels is ', labels.shape)\n",
    "\n",
    "dict_reviewLabels = {'review': dataMatrix,'labels': labelsMatrix}\n",
    "df_reviewLabels = pd.DataFrame(dict_reviewLabels)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "macronum=sorted(set(df_reviewLabels['labels']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "df_reviewLabels.iloc[:,0]=df_reviewLabels.iloc[:,0].apply(fun)\n",
    "df_reviewLabels.head(2)\n",
    "\n",
    "\n",
    "\n",
    "print(df_reviewLabels.shape)\n",
    "# a list contains each review as a list \n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "\n",
    "for i in range(len(df_reviewLabels)):\n",
    "    balanced_texts.append(df_reviewLabels.iloc[i,1])\n",
    "    balanced_labels.append(df_reviewLabels.iloc[i,0])\n",
    " \n",
    "\n",
    "#MAX_NB_WORDS = len(w2v_model.wv.vocab)\n",
    "tokenizer = Tokenizer(num_words=2000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \")#20000\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "x = pad_sequences(sequences, maxlen=200)#300\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(balanced_labels))\n",
    "#y = df_reviewLabels['labels'].values\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, GlobalMaxPooling1D, Conv1D, Dropout, MaxPooling1D, Dense, Embedding, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "# Build embedding layers with weights initialized from each model\n",
    "googlenews_w2v_size = 300\n",
    "googlenews_w2v_matrix = np.zeros((len(word_index) + 1, googlenews_w2v_size))\n",
    "for word,i in word_index.items():\n",
    "    try:\n",
    "        if word in w2v_model.vocab:\n",
    "            googlenews_w2v_matrix[i] = w2v_model[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "googlenews_w2v_emb = Embedding(len(word_index)+1,\n",
    "\n",
    "                            googlenews_w2v_size,\n",
    "\n",
    "                            weights=[googlenews_w2v_matrix],\n",
    "\n",
    "                            input_length=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * x.shape[0])\n",
    "\n",
    "x_train = x[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = x[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing words is  894\n",
      "Found 9687 word vectors.\n",
      "Vocalbulary size 3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordsVectors = []\n",
    "notFoundwords = []\n",
    "for word in vocabulary:\n",
    "    try:\n",
    "        vector = w2v_model[word]\n",
    "        wordsVectors.append(vector)\n",
    "    except Exception as e:\n",
    "        notFoundwords.append(word)\n",
    "        wordsVectors.append(np.random.uniform(-0.25,0.25,300))  \n",
    "\n",
    "#del w2v_model\n",
    "wordsVectors = np.asarray(wordsVectors)\n",
    "print('The number of missing words is ', len(notFoundwords))\n",
    "\n",
    "print('Found %s word vectors.' % len(wordsVectors))\n",
    "words = list(w2v_model.wv.vocab)\n",
    "print('Vocalbulary size %s' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>we stay at hilton for 4 nights last march it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this is a stunning hotel in an excellent locat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                             review\n",
       "0       1  we stay at hilton for 4 nights last march it w...\n",
       "1       1  this is a stunning hotel in an excellent locat..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "def convertFileToIndexArray(filePath):\n",
    "    doc = np.zeros(MAX_SEQ_LENGTH, dtype='int32')\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        indexCounter = 0\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            words = cleanedLine.split(' ')\n",
    "            for word in words:\n",
    "                doc[indexCounter] = vocabulary.index(word)\n",
    "                indexCounter = indexCounter + 1\n",
    "                if (indexCounter >= MAX_SEQ_LENGTH):\n",
    "                    break\n",
    "            if (indexCounter >= MAX_SEQ_LENGTH):\n",
    "                break\n",
    "    return doc\n",
    "\n",
    "def convertFileToArray(filePath):\n",
    "    s = \"\"\n",
    "    with open(filePath, \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            cleanedLine = clean_str(line)\n",
    "            cleanedLine = cleanedLine.strip()\n",
    "            cleanedLine = cleanedLine.lower()\n",
    "            s += cleanedLine\n",
    "    return s\n",
    "\n",
    "totalFiles = len(truthful_reviews_link) + len(deceptive_reviews_link)\n",
    "idsMatrix = np.ndarray(shape=(totalFiles, MAX_SEQ_LENGTH), dtype='int32')\n",
    "#dataMatrix = np.ndarray(shape=(totalFiles,1),dtype='object')\n",
    "dataMatrix = []\n",
    "labels = np.ndarray(shape=(totalFiles, 2), dtype='int32')\n",
    "labelsMatrix = []\n",
    "counter = 0\n",
    "'''\n",
    "for filePath in truthful_reviews_link:\n",
    "    idsMatrix[counter] = convertFileToIndexArray(filePath)\n",
    "    counter = counter + 1\n",
    "\n",
    "for filePath in deceptive_reviews_link:\n",
    "    idsMatrix[counter] = convertFileToIndexArray(filePath)\n",
    "    counter = counter + 1\n",
    "'''\n",
    "for filePath in truthful_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(1)\n",
    "    #dataMatrix[counter] = convertFileToArray(filePath)\n",
    "    #counter = counter + 1\n",
    "\n",
    "for filePath in deceptive_reviews_link:\n",
    "    dataMatrix.append(convertFileToArray(filePath))\n",
    "    labelsMatrix.append(0)\n",
    "    #dataMatrix[counter] = convertFileToArray(filePath)\n",
    "    #counter = counter + 1\n",
    "\n",
    "#labels[0:len(truthful_reviews_link)] = np.array([1, 0])\n",
    "#labels[len(truthful_reviews_link):totalFiles] = np.array([0, 1])\n",
    "\n",
    "#print('The shape of the data matrix is ', dataMatrix.shape)\n",
    "#print('The shape of the labels is ', labels.shape)\n",
    "\n",
    "dict_reviewLabels = {'review': dataMatrix,'labels': labelsMatrix}\n",
    "df_reviewLabels = pd.DataFrame(dict_reviewLabels)\n",
    "df_reviewLabels.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>we stay at hilton for 4 nights last march it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this is a stunning hotel in an excellent locat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                             review\n",
       "0       1  we stay at hilton for 4 nights last march it w...\n",
       "1       1  this is a stunning hotel in an excellent locat..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macronum=sorted(set(df_reviewLabels['labels']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "df_reviewLabels.iloc[:,0]=df_reviewLabels.iloc[:,0].apply(fun)\n",
    "df_reviewLabels.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list contains each review as a list \n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "\n",
    "for i in range(len(df_reviewLabels)):\n",
    "    balanced_texts.append(df_reviewLabels.iloc[i,1])\n",
    "    balanced_labels.append(df_reviewLabels.iloc[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "'''            \n",
    "# Model Hyperparameters\n",
    "SENTENCE_PER_REVIEW = 16\n",
    "WORDS_PER_SENTENCE = 10\n",
    "EMBEDDING_DIM = 300\n",
    "FILTER_WIDTHS_SENT_CONV = np.array([3, 4, 5])\n",
    "NUM_FILTERS_SENT_CONV = 100\n",
    "FILTER_WIDTHS_DOC_CONV = np.array([3, 4, 5])\n",
    "NUM_FILTERS_DOC_CONV = 100\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT_KEEP_PROB = 0.5\n",
    "L2_REG_LAMBDA = 0.0\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "EVALUATE_EVERY = 100   # Evaluate model on the validation set after 100 steps\n",
    "CHECKPOINT_EVERY = 100 # Save model after each 200 steps\n",
    "NUM_CHECKPOINTS = 5    # Keep only the 5 most recents checkpoints\n",
    "LEARNING_RATE = 1e-3   # The learning rate\n",
    "'''\n",
    "\n",
    "# Model Hyperparameters\n",
    "SENTENCE_PER_REVIEW = 16\n",
    "WORDS_PER_SENTENCE = 10\n",
    "EMBEDDING_DIM = 300\n",
    "FILTER_WIDTHS_SENT_CONV = np.array([3, 4, 5])\n",
    "NUM_FILTERS_SENT_CONV = 100\n",
    "FILTER_WIDTHS_DOC_CONV = np.array([3, 4, 5])\n",
    "NUM_FILTERS_DOC_CONV = 100\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT_KEEP_PROB = 0.5\n",
    "L2_REG_LAMBDA = 0.0\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "EVALUATE_EVERY = 100   # Evaluate model on the validation set after 100 steps\n",
    "CHECKPOINT_EVERY = 100 # Save model after each 200 steps\n",
    "NUM_CHECKPOINTS = 5    # Keep only the 5 most recents checkpoints\n",
    "LEARNING_RATE = 1e-3   # The learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_NB_WORDS = len(w2v_model.wv.vocab)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)#20000\n",
    "tokenizer.fit_on_texts(dataMatrix)\n",
    "sequences = tokenizer.texts_to_sequences(dataMatrix)\n",
    "data = pad_sequences(sequences, maxlen=200)#300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9682 unique tokens.\n",
      "Shape of data tensor: (1600, 200)\n",
      "Shape of label tensor: (1600, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "labels = to_categorical(np.asarray(labelsMatrix))\n",
    "#labels = np.array(balanced_labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18457031 -0.15917969  0.25195312 ... -0.01086426 -0.15527344\n",
      "   0.20703125]\n",
      " [-0.18945312  0.18164062  0.3359375  ...  0.11669922  0.05688477\n",
      "   0.10546875]\n",
      " [-0.05834961  0.06787109 -0.05395508 ...  0.08447266  0.02307129\n",
      "   0.24316406]\n",
      " ...\n",
      " [ 0.07568359  0.2265625   0.18652344 ...  0.14355469 -0.08935547\n",
      "   0.28320312]\n",
      " [-0.0168457   0.09521484 -0.13085938 ... -0.11865234  0.26367188\n",
      "   0.15820312]\n",
      " [ 0.24023438  0.0025177  -0.10107422 ...  0.31445312 -0.28710938\n",
      "   0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "print(wordsVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:00<00:00, 7866.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 9603 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from string import punctuation, ascii_lowercase\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# replace urls\n",
    "re_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n",
    "                    .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n",
    "                    re.MULTILINE|re.UNICODE)\n",
    "# replace ips\n",
    "re_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n",
    "\n",
    "# setup tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def text_to_wordlist(text, lower=False):\n",
    "    # replace URLs\n",
    "    #text = re_url.sub(\"URL\", text)\n",
    "    \n",
    "    # replace IPs\n",
    "    #text = re_ip.sub(\"IPADDRESS\", text)\n",
    "    \n",
    "    # Tokenize\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # optional: lower case\n",
    "    if lower:\n",
    "        text = [t.lower() for t in text]\n",
    "    \n",
    "    # Return a list of words\n",
    "    vocab.update(text)\n",
    "    return text\n",
    "\n",
    "def process_text(list_sentences, lower=False):\n",
    "    textList = []\n",
    "    for text in tqdm(list_sentences):\n",
    "        txt = text_to_wordlist(text, lower=lower)\n",
    "        textList.append(txt)\n",
    "    return textList\n",
    "\n",
    "\n",
    "#list_sentences_train = list(train_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n",
    "#list_sentences_test = list(test_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n",
    "\n",
    "texts = process_text(balanced_texts)\n",
    "print(\"The vocabulary contains {} unique tokens\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/nhumair/.conda/envs/my_env/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#code from yelp dataset preprocessing\n",
    "\n",
    "#prepare embedding matrix\n",
    "EMBEDDING_DIM = 300\n",
    "#num_words = min(20000, len(word_index)) + 1\n",
    "#embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > 20000:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "'''     \n",
    "      \n",
    "MAX_NB_WORDS = len(w2v_model.wv.vocab)        \n",
    "WV_DIM = 100\n",
    "word_vectors = w2v_model.wv\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "\n",
    "sequences = [[word_index.get(t, 0) for t in text]\n",
    "             for text in texts[:len(texts)]]\n",
    "#test_sequences = [[word_index.get(t, 0)  for t in comment] \n",
    "                  #for comment in comments[len(list_sentences_train):]]\n",
    "\n",
    "# pad\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "#MAX_NB_WORDS = len(w2v_model.wv.vocab)\n",
    "nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))\n",
    "# we initialize the matrix with random numbers\n",
    "#wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\n",
    "#wv_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "wv_matrix = np.random.random((len(word_index) + 1, WV_DIM))\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "MAX_SEQ_LENGTH = 200\n",
    "embedding_layer = Embedding(len(word_index)+1, WV_DIM,weights=[wv_matrix],\n",
    "                         input_length=MAX_SEQ_LENGTH,trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 200)\n"
     ]
    }
   ],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    Original taken from https://github.com/dennybritz/cnn-text-classification-tf/blob/master/data_helpers.py\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "\n",
    "#batches = batch_iter(list(zip(x_train, y_train)), 64, 10)\n",
    "\n",
    "\n",
    "batches_x = batch_iter(\n",
    "            list(data), 32, 10)\n",
    "batches_y = batch_iter(list(labels),32,10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_gen(top_dim):\n",
    "    \"\"\"\n",
    "    Generator to yield batches of two inputs (per sample) with shapes top_dim and \n",
    "    bot_dim along with their labels.\n",
    "    \"\"\"\n",
    "    batch_size = 32\n",
    "    while True:\n",
    "        top_batch = []\n",
    "        #bot_batch = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            # Create random arrays\n",
    "            rand_pix = np.random.randint(100, 256)\n",
    "            #top_img = np.full(top_dim, rand_pix)\n",
    "            #bot_img = np.full(bot_dim, rand_pix)\n",
    "            top_img = np.full(top_dim,fill_value=rand_pix)\n",
    "            \n",
    "            # Set a label\n",
    "            label = np.random.choice([0, 1])\n",
    "            batch_labels.append(label)\n",
    "\n",
    "            # Pack each input image separately\n",
    "            top_batch.append(top_img)\n",
    "            #bot_batch.append(bot_img)\n",
    "\n",
    "        yield np.array(top_batch), np.array(batch_labels)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv1d_20: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-972dc7bbcb0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msequence_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv1d_20: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "from keras.layers import Input, GlobalMaxPooling1D, Conv1D, Dropout, MaxPooling1D, Dense\n",
    "from keras.models import Model\n",
    "sequence_input = Input(shape=(x_train.shape[0],MAX_SEQ_LENGTH), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(macronum), activation='softmax')(x)\n",
    "#preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "'''\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
    "'''\n",
    "\n",
    "# split data into training and testing sets\n",
    "#_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3)\n",
    "#for batch in batches:\n",
    "            #x_batch, y_batch = zip(*batch)\n",
    "    \n",
    "my_gen = data_gen(x_train.shape)\n",
    "#history = model.fit(batches_x, batches_y,batch_size=32,epochs=250,validation_data=(x_val, y_val))\n",
    "history = model.fit_generator(my_gen,steps_per_epoch=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_iter(data, labels, batch_size, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X, y = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(macronum), activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "    if mode == 'fit':\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n",
    "    else:\n",
    "   \"\"\" \n",
    "train_steps, train_batches = batch_iter(x_train, y_train, batch_size)\n",
    "valid_steps, valid_batches = batch_iter(x_test, y_test, batch_size)\n",
    "model.fit_generator(train_batches, train_steps, epochs=1, validation_data=valid_batches, validation_steps=valid_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.6625\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "python_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
